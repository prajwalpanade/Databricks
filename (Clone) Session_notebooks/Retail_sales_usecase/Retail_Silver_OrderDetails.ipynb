{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5205663b-3357-4342-892a-3257e44cdc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Cell 1: Define paths for bronze and silver data\n",
    "bronze_path = '/mnt/Prajwal/Retail_sales_usecase/bronzeODetails'\n",
    "silver_path = '/mnt/Prajwal/Retail_sales_usecase/Silver/silverODetails'\n",
    "\n",
    "# Cell 2: Read data from the bronze path\n",
    "df = spark.read.format(\"parquet\").load(bronze_path)\n",
    "\n",
    "# Cell 3: Define UDF to parse dates and convert to timestamp\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime\n",
    "\n",
    "def parse_with_year(date_str):\n",
    "    try:\n",
    "        if not date_str:\n",
    "            return None\n",
    "        \n",
    "        # Remove leading and trailing spaces\n",
    "        date_str = date_str.strip()\n",
    "        \n",
    "        # Try parsing with two-digit year format\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, '%m/%d/%y %H:%M')\n",
    "        except ValueError:\n",
    "            # If it fails, try parsing with four-digit year format\n",
    "            date_obj = datetime.strptime(date_str, '%m/%d/%Y %H:%M')\n",
    "\n",
    "        # Adjust the year if it is greater than the current year + 5\n",
    "        if date_obj.year > datetime.now().year + 5:\n",
    "            date_obj = date_obj.replace(year=date_obj.year - 100)\n",
    "            \n",
    "        return date_obj\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Register the UDF\n",
    "parse_udf = udf(parse_with_year, TimestampType())\n",
    "\n",
    "# Apply the UDF to the order_date column\n",
    "df = df.withColumn(\"order_date\", parse_udf(col(\"order_date\")))\n",
    "\n",
    "# Cell 4: Data cleaning and transformation\n",
    "from pyspark.sql.functions import col, regexp_replace, trim, monotonically_increasing_id\n",
    "\n",
    "# Renaming columns: order_value to order_amount and branch_code to store_code\n",
    "df = df.withColumnRenamed(\"order_value\", \"order_amount\").withColumnRenamed(\"branch_code\", \"store_code\")\n",
    "\n",
    "# Fill missing values for state, country, and order_channel\n",
    "df = df.fillna(value={\"state\": \"Unknown\", \"country\": \"Unknown\", \"order_channel\": \"Unknown\"})\n",
    "\n",
    "# Cast order_amount to float and round to 2 decimal places\n",
    "df = df.withColumn(\"order_amount\", col(\"order_amount\").cast(\"float\").cast(\"decimal(10,2)\"))\n",
    "\n",
    "# Remove special characters from store_code column\n",
    "df = df.withColumn(\"store_code\", regexp_replace(col(\"store_code\"), r\"[^a-zA-Z0-9]\", \"\"))\n",
    "\n",
    "# Trim spaces for order_channel, state, and country columns\n",
    "df = df.withColumn(\"order_channel\", trim(col(\"order_channel\")))\n",
    "df = df.withColumn(\"state\", trim(col(\"state\")))\n",
    "df = df.withColumn(\"country\", trim(col(\"country\")))\n",
    "\n",
    "# Define valid mapping (standardize the value) for order_channel\n",
    "mapping = {\"online\": \"Online\",\n",
    "           \"instore\": \"In-Store\",\n",
    "           \"phone\": \"Phone\"\n",
    "}\n",
    "df = df.replace(mapping, subset=[\"order_channel\"])\n",
    "\n",
    "# Add surrogate key for orders\n",
    "df = df.withColumn(\"order_sk\", monotonically_increasing_id())\n",
    "\n",
    "# Writing data to silver path\n",
    "df.write.mode(\"overwrite\").format(\"delta\").save(silver_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51d7fd61-660e-49d1-bf69-9ec6ec573faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read loaded file from Silver layer\n",
    "df = spark.read.format(\"delta\").load(silver_path)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail_Silver_OrderDetails",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
