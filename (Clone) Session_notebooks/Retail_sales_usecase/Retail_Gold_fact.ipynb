{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b580e3d-1513-4e8e-8dbf-9bfcbe43d33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Referential Integrity Check Before fact_sales Merge\n",
    "# Before merging fact_sales, check if all customer_id values exist in dim_customer.\n",
    "# Identify orphaned customer_id values in fact_sales\n",
    "# If any records appear in this query, it means customer_id values exist in fact_sales but not in dim_customer.\n",
    "# You can reject, log, or hold these records until their dimensions arrive.\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load the fact_sales and dim_customer tables\n",
    "\n",
    "orders_df = spark.read.format(\"delta\").load(\"/mnt/Prajwal/Retail_sales_usecase/Silver/SilverCDetails\")\n",
    "dim_customer_df = spark.table(\"retail.customerdimension\")\n",
    "\n",
    "\n",
    "display(orders_df)\n",
    "display(dim_customer_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56737dcd-66a8-40b2-9226-574479e2505a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform a left join to identify orphaned customer_id in fact_sales\n",
    "orphaned_customer_ids = orders_df.join(\n",
    "    dim_customer_df, \n",
    "    orders_df.customer_id == dim_customer_df.customer_id, \n",
    "    how='left'\n",
    ").filter(dim_customer_df.customer_id.isNull()) \\\n",
    "  .select(orders_df.customer_id)\n",
    "\n",
    "  # Get distinct orphaned customer_id values\n",
    "orphaned_customer_ids_distinct = orphaned_customer_ids.distinct()\n",
    "\n",
    "# Show orphaned customer_id values\n",
    "orphaned_customer_ids_distinct.show()\n",
    "\n",
    "# Count orphaned records if needed\n",
    "orphaned_count = orphaned_customer_ids_distinct.count()\n",
    "print(f\"Number of orphaned records: {orphaned_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44d9d203-6cd7-46de-bc6e-203c0881dcf1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the fact_sales and dim_customer tables\n",
    "\n",
    "orders_df = spark.read.format(\"delta\").load(\"/mnt/Prajwal/Retail_sales_usecase/Silver/SilverPDetails\")\n",
    "dim_product_df = spark.table(\"retail.productdimension\")\n",
    "\n",
    "# Perform a left join to identify orphaned customer_id in fact_sales\n",
    "orphaned_product_ids = orders_df.join(\n",
    "    dim_product_df, \n",
    "    orders_df.product_id == dim_product_df.product_id, \n",
    "    how='left'\n",
    ").filter(dim_product_df.product_id.isNull()) \\\n",
    "  .select(orders_df.product_id)\n",
    "\n",
    "# Get distinct orphaned customer_id values\n",
    "orphaned_product_ids_distinct = orphaned_product_ids.distinct()\n",
    "\n",
    "# Show orphaned customer_id values\n",
    "orphaned_product_ids_distinct.show()\n",
    "\n",
    "# Count orphaned records if needed\n",
    "orphaned_count = orphaned_product_ids_distinct.count()\n",
    "print(f\"Number of orphaned records: {orphaned_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c5414f6-c299-4bb3-8587-f31402a3f0a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **FACT **TABLE****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7088e94-aa6b-4f3e-b5d2-48a6890a3ba1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Broadcast Join (Performance Boost) - broadcast(product_df) - Avoids expensive shuffle joins\n",
    "# Late Arriving Data Handling - fillna(-1) for customer_id - Assigns a default key for missing dimensions\n",
    "# Partitioning Strategy - repartition(\"date_key\") - Partitions data efficiently for faster reads\n",
    "# ZORDER Optimization - OPTIMIZE FactSales ZORDER BY (customer_id, product_id); - Speeds up queries on frequent filters\n",
    "# Caching - Avoid recomputation of joins using persist, Speed up writes and Free up memory\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# ---------------------------\n",
    "# Load Silver Data\n",
    "# ---------------------------\n",
    "orders_df = spark.read.format(\"delta\").load(\"/mnt/Prajwal/Retail_sales_usecase/Silver/silverODetails\")\n",
    "products_df = spark.read.format(\"delta\").load(\"/mnt/Prajwal/Retail_sales_usecase/Silver/SilverPDetails\")\n",
    "customer_df = spark.read.format(\"delta\").load(\"/mnt/Prajwal/Retail_sales_usecase/Silver/SilverCDetails\")\n",
    "customerInsight_df = spark.read.format(\"delta\").load(\"/mnt/Prajwal/Retail_sales_usecase/Silver/SilverCInsights\")\n",
    "\n",
    "# ---------------------------\n",
    "# Incremental Load: Exclude already processed order_ids\n",
    "# ---------------------------\n",
    "try:\n",
    "    gold_fact_df = spark.read.format(\"delta\").load(\"/mnt/Prajwal/Retail_sales_usecase/Gold\")\n",
    "    existing_order_ids = gold_fact_df.select(\"order_id\").distinct()\n",
    "    orders_df = orders_df.join(existing_order_ids, on=\"order_id\", how=\"left_anti\")\n",
    "except:\n",
    "    print(\"Gold fact table doesn't exist. Full load will be performed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a475f1e7-6915-4e74-b82d-0631002f37c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Broadcast Join Optimization for Small Dimensions\n",
    "# ---------------------------\n",
    "products_df = broadcast(products_df)  # Assuming product table is small\n",
    "\n",
    "# ---------------------------\n",
    "# Handle Late Arriving Dimensions\n",
    "# ---------------------------\n",
    "# Assign a default surrogate key (-1) for missing customers\n",
    "customer_df = customer_df.withColumn(\"customer_id_late\", col(\"customer_id\")).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60a3b7aa-85ed-4342-badc-3a796397c460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------------------\n",
    "# Repartition Orders Data to Reduce Shuffle Before Join\n",
    "# Repartition Order Data to Reduce Shuffle Before Join\n",
    "# Instead of Spark randomly distributing data, it ensures that all records with the same customer_id go to the same partition.\n",
    "# This reduces the number of data movements (shuffles) needed when joining with customer_df.\n",
    "# Prepares data for efficient joins\n",
    "# Tuning the number of partitions (100, 10 etc) depends on data size\n",
    "# ---------------------------\n",
    "orders_df = orders_df.repartition(100, \"customer_id\")  # Adjust partitions as per data size\n",
    "\n",
    "\n",
    "def parse_with_year(date_str):\n",
    "    try:\n",
    "        if not date_str:\n",
    "            return None\n",
    "        \n",
    "        # Remove leading and trailing spaces\n",
    "        date_str = date_str.strip()\n",
    "        \n",
    "        # Try parsing with two-digit year format\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, '%m/%d/%y %H:%M')\n",
    "        except ValueError:\n",
    "            # If it fails, try parsing with four-digit year format\n",
    "            date_obj = datetime.strptime(date_str, '%m/%d/%Y %H:%M')\n",
    "\n",
    "        # Adjust the year if it is greater than the current year + 5\n",
    "        if date_obj.year > datetime.now().year + 5:\n",
    "            date_obj = date_obj.replace(year=date_obj.year - 100)\n",
    "            \n",
    "        return date_obj\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Register the UDF\n",
    "parse_udf = udf(parse_with_year, TimestampType())\n",
    "\n",
    "# ---------------------------\n",
    "# Convert order_date to date format first (before any further transformation)\n",
    "# ---------------------------\n",
    "orders_df = orders_df.withColumn(\"order_date\", parse_udf(col(\"order_date\")))\n",
    "\n",
    "display(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2c51834-2206-40b3-8b94-22b7056078d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "x# ---------------------------\n",
    "# Join with Product info\n",
    "# ---------------------------\n",
    "orders_enriched = orders_df.join(\n",
    "    products_df.select(\"product_id\", \"price\", \"in_stock\"),\n",
    "    on=\"product_id\", how=\"left\"\n",
    ")\n",
    "\n",
    "display(orders_enriched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c16888-5306-4bad-821f-096d88879fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Derived columns\n",
    "# ---------------------------\n",
    "orders_enriched = orders_enriched.withColumn(\"order_value\", col(\"order_amount\").cast(\"decimal(10,2)\")) \\\n",
    "    .withColumn(\"quarter\", quarter(\"order_date\")) \\\n",
    "    .withColumn(\"year\", year(\"order_date\")) \\\n",
    "    .withColumn(\"quarter_year\", concat(lit(\"Q\"), col(\"quarter\"), lit(\"-\"), col(\"year\"))) \\\n",
    "    .withColumn(\"in_stock_flag\", when(col(\"in_stock\") == \"Yes\", \"Y\").otherwise(\"N\"))  # Removed order_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e922edc-22f7-48d3-92d2-16d6e0694d9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " #---------------------------\n",
    "# Rename customer_df columns to avoid ambiguity, remove state and country\n",
    "# ---------------------------\n",
    "customer_df_renamed = customer_df.select(\n",
    "    \"customer_id\"\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Join customer dimension\n",
    "# ---------------------------\n",
    "fact_df = orders_enriched.join(\n",
    "    customer_df_renamed,\n",
    "    on=\"customer_id\", how=\"left\"\n",
    ")\n",
    "display(fact_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c08f37-25d7-4daf-a0b1-a2b0d9505b29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# High Value Flag\n",
    "# ---------------------------\n",
    "customer_total_spend = fact_df.groupBy(\"customer_id\").agg(sum(\"order_value\").alias(\"total_spent\"))\n",
    "fact_df = fact_df.join(customer_total_spend, on=\"customer_id\", how=\"left\") \\\n",
    "    .withColumn(\"Is_High_Value_Customer\", when(col(\"total_spent\") > 10000, \"Y\").otherwise(\"N\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a450a24-4491-4a45-ad02-96ae3725bee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Generate Surrogate Key\n",
    "# ---------------------------\n",
    "fact_df = fact_df.withColumn(\"Sales_Key\", monotonically_increasing_id())\n",
    "\n",
    "# ---------------------------\n",
    "# Select Final Columns (Removed order_count)\n",
    "# ---------------------------\n",
    "fact_final = fact_df.select(\n",
    "    \"Sales_Key\", \"customer_id\", \"product_id\", \"order_id\", \"order_date\", \"order_channel\",\n",
    "    \"store_code\", \"order_value\", \"price\", \"quarter\", \"quarter_year\", \n",
    "    \"Is_High_Value_Customer\", \"in_stock_flag\"\n",
    ")\n",
    "display(fact_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e876b5fa-47e4-48c5-b0c2-9421d496df90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Persist the fact dataframe\n",
    "# ---------------------------\n",
    "# Once you've performed the joins with customer_df, insights_df, and product_df, persisting fact_df will help avoid recomputation:\n",
    "# Add persist() after joins but before transformations like date conversion . It will help avoid recomputation\n",
    "\n",
    "# Persist in MEMORY_AND_DISK (best for iterative queries)\n",
    "fact_df = fact_df.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "print(\"Fact table persisted after joins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "269749f8-fbbc-47ad-bdbe-5fefc17b5915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Partition Strategy: Convert order_date to date_key (YYYYMMDD as INT)\n",
    "# ---------------------------\n",
    "# Convert order_date to date format first\n",
    "# to_date(col(\"order_date\")) works only for YYYY-MM-DD format.\n",
    "\n",
    "fact_df = fact_df.withColumn(\"date_key\", expr(\"CAST(date_format(order_date, 'yyyyMMdd') AS INT)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2783d6db-4290-4808-bb07-f822d8b8ead8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Add Effective Date and Expiry Info\n",
    "# ---------------------------\n",
    "fact_df = fact_df.withColumn(\"effective_date\", current_timestamp()) \\\n",
    "                 .withColumn(\"expiry_date\", lit(None)) \\\n",
    "                 .withColumn(\"is_current\", lit(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aba4c54-fb67-4515-ae68-2987e4a71849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Coalesce to reduce small files before writing\n",
    "# coalesce() Used after transformations (before writing to disk) to reduce unnecessary small files.\n",
    "# ---------------------------\n",
    "fact_df = fact_df.coalesce(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b97596a1-ff44-4c41-a7bb-bd49bef49fa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Cache to speed up the write operation\n",
    "# Before writing to Delta Gold Table, cache it to speed up the write operation:\n",
    "# ---------------------------\n",
    "fact_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcecc397-218c-44c4-be71-62a902139f43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " #---------------------------\n",
    "# Write to Gold Layer\n",
    "# ---------------------------\n",
    "# Checks if the Gold Layer Delta table already exists.\n",
    "# If it exists, performs an upsert (MERGE) based on order_id to update or insert records.\n",
    "# If it doesn't exist, writes the full DataFrame as a new partitioned Delta table by quarter_year.\n",
    "\n",
    "gold_path = \"/mnt/Prajwal/Retail_sales_usecase/Gold\"\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, gold_path):\n",
    "    delta_table = DeltaTable.forPath(spark, gold_path)\n",
    "\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        fact_final.alias(\"source\"),\n",
    "        \"target.order_id = source.order_id\"\n",
    "    ).whenMatchedUpdateAll() \\\n",
    "     .whenNotMatchedInsertAll() \\\n",
    "     .execute()\n",
    "else:\n",
    "    fact_final.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"quarter_year\") \\\n",
    "        .save(gold_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd78355-6b4c-4f53-abd9-04731f6dbcaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Backfilling\n",
    "# Identifies records in the fact table where customer_id = -1 (i.e., unknown at load time).\n",
    "# Backfills the missing customer_id when the corresponding surrogate key is matched in the dimension.\n",
    "# Ensures data consistency when late-arriving dimension records show up.\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Define paths or use table names if registered in metastore\n",
    "fact_table = DeltaTable.forName(spark, \"retail.Fact_Sales\")\n",
    "dim_customer_df = spark.table(\"retail.customerdimension\")\n",
    "\n",
    "# Count rows where customer_id = -1 before merge\n",
    "before_count = fact_table.toDF().filter(col(\"customer_id\") == \"-1\").count()\n",
    "\n",
    "# Perform the merge to backfill missing customer_id\n",
    "fact_table.alias(\"fact\") \\\n",
    "    .merge(\n",
    "        dim_customer_df.alias(\"dim\"),\n",
    "        \"fact.customer_id = '-1' AND fact.customer_id = dim.customer_id\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdate(set={\n",
    "        \"fact.customer_id\": \"dim.customer_id\"\n",
    "    }) \\\n",
    "    .whenNotMatchedInsert(values={\n",
    "        \"fact.customer_id\": \"dim.customer_id\"  # You can specify other insert logic if needed\n",
    "    }) \\\n",
    "    .execute()\n",
    "\n",
    "# Count rows where customer_id was updated after the merge\n",
    "after_count = fact_table.toDF().filter(col(\"customer_id\") == \"-1\").count()\n",
    "\n",
    "# Calculate the number of affected rows\n",
    "updated_rows = before_count - after_count\n",
    "\n",
    "# Print affected counts\n",
    "print(f\"Records updated: {updated_rows}\")\n",
    "print(f\"Total affected records (before merge): {before_count}\")\n",
    "print(f\"Remaining records with customer_id = -1 (after merge): {after_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5632e4a3-74f2-4a6c-8122-9b77f56c509d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail_Gold_fact",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
