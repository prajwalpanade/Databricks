{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a788461-cfd8-4980-b781-3d3d2c7fc0e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#importing important function\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Defining paths\n",
    "source_path = \"/mnt/Prajwal/Retail_sales_usecase/source_files\"\n",
    "bronze_path = \"/mnt/Prajwal/Retail_sales_usecase/bronzeCDetails\"\n",
    "\n",
    "# Defining log path\n",
    "log_path = '/mnt/Prajwal/Retail_sales_usecase/logs'\n",
    "\n",
    "files = ['CDetails_day1.csv']\n",
    "\n",
    "# Define the enhanced Log Table Schema\n",
    "log_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"log_level\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"file_type\", StringType(), True),\n",
    "    StructField(\"file_size_kb\", DoubleType(), True),\n",
    "    StructField(\"file_mod_time\", TimestampType(), True),\n",
    "    StructField(\"record_count\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"processing_time_sec\", IntegerType(), True),\n",
    "    StructField(\"processed_by\", StringType(), True),\n",
    "    StructField(\"processed_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Function to log message with additional metadata\n",
    "def log_message(source_file, file_type, file_size_kb, file_mod_time, record_count, status, processing_time_sec, processed_by, message):\n",
    "    log_entry = [(datetime.now(), \"INFO\", message, file_type, file_size_kb, file_mod_time, record_count, status, processing_time_sec, processed_by, datetime.now())]\n",
    "    log_df = spark.createDataFrame(log_entry, log_schema)\n",
    "    log_df.write.format(\"delta\").mode(\"append\").save(log_path)\n",
    "    print(f\"{datetime.now()} - [{message}]\")\n",
    "\n",
    "# Function to process files in a directory and log metadata\n",
    "def process_files_in_directory(files, bronze_path):\n",
    "    try:\n",
    "        # List all the files in the directory\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(source_path, file_name)\n",
    "\n",
    "            # Log file details\n",
    "            file_extension = file_name.split(\".\")[-1]\n",
    "            file_name = file_name.split(\".\")[0]\n",
    "            file_info = dbutils.fs.ls(file_path)[0]\n",
    "            file_size_kb = file_info.size / 1024\n",
    "            file_mod_time = datetime.fromtimestamp(file_info.modificationTime / 1000)\n",
    "            processed_by = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "\n",
    "            if file_extension == \"csv\":\n",
    "                file_type = \"CSV\"\n",
    "            elif file_extension == \"txt\":\n",
    "                file_type = \"TXT\"\n",
    "            else:\n",
    "                file_type = \"UNKNOWN\"\n",
    "                log_message(file_path, file_type, file_size_kb, file_mod_time, 0, \"FAILED\", 0, processed_by, f\"Unsupported file format: {file_extension}. Only CSV and TXT files are supported.\")\n",
    "                continue\n",
    "\n",
    "            # Read the file\n",
    "            start_time = time.time()\n",
    "            if file_type == \"CSV\":\n",
    "                log_message(file_path, file_type, file_size_kb, file_mod_time, None, \"PROCESSING\", 0, processed_by, f\"Reading CSV File {file_name}\")\n",
    "                df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "            elif file_type == \"TXT\":\n",
    "                log_message(file_path, file_type, file_size_kb, file_mod_time, None, \"PROCESSING\", 0, processed_by, f\"Reading TXT File {file_name}\")\n",
    "                df = spark.read.format(\"text\").load(file_path)\n",
    "\n",
    "            #adding ingestion time\n",
    "            df = df.withColumn(\"ingest_time\", F.current_timestamp())\n",
    "\n",
    "            # Record count and processing time\n",
    "            record_count = df.count()\n",
    "            processing_time_sec = int(time.time() - start_time)\n",
    "            log_message(file_path, file_type, file_size_kb, file_mod_time, record_count, \"PROCESSING\", processing_time_sec, processed_by, f\"Successfully read {record_count} records from {file_path}\")\n",
    "\n",
    "            # Writing data to output path\n",
    "            log_message(file_path, file_type, file_size_kb, file_mod_time, record_count, \"PROCESSING\", processing_time_sec, processed_by, f\"Writing data to {bronze_path}\")\n",
    "            df.write.format(\"parquet\").mode(\"append\").save(bronze_path + file_name)\n",
    "            \n",
    "\n",
    "            # Final status\n",
    "            log_message(file_path, file_type, file_size_kb, file_mod_time, record_count, \"COMPLETED\", processing_time_sec, processed_by, f\"Successfully processed {file_path}\")\n",
    "    except Exception as e:\n",
    "        processing_time_sec = int(time.time() - start_time)\n",
    "        log_message(file_path, file_type, file_size_kb, file_mod_time, 0, \"FAILED\", processing_time_sec, processed_by, f\"Error processing file {file_path}: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "process_files_in_directory(files, bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a6dcaaa-2e19-406a-b69a-93f4abaa2a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the source file into a DataFrame\n",
    "\n",
    "source_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(source_path + \"/CDetails_day1.csv\")\n",
    "\n",
    "# Displaying the DataFrame\n",
    "display(source_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e80e17-e4fa-4c1e-b62f-2b4f4ae4aece",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_path = \"/mnt/Prajwal/Retail_sales_usecase/bronzeCDetails\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e523283-53da-4d45-bd29-1e66f9c11104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").mode(\"append\").option(\"mergeSchema\", \"true\").save(bronze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a820280b-81ad-40ba-a476-67e1d3688494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Reading the Parquet file from bronze_path into a DataFrame\n",
    "bronze_df = spark.read.parquet(bronze_path)\n",
    "\n",
    "# Displaying the DataFrame\n",
    "display(bronze_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8a5b713-5db6-4297-9347-f8126d639722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail_Day1_customer_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
