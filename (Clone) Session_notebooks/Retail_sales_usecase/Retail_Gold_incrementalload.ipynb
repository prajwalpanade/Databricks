{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43670596-2ed1-42c2-b820-75e59c95cddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Paths\n",
    "silver_path = \"/mnt/Prajwal/Retail_sales_usecase/Silver/\"\n",
    "gold_path = \"/mnt/Prajwal/Retail_sales_usecase/Gold\"\n",
    "\n",
    "# Load Day 1 Data\n",
    "df_day1 = spark.read.format(\"delta\").load(\n",
    "    silver_path + \"SilverCDetails_day1/ingestion_time=2025-04-30 06%3A53%3A33/\"\n",
    ")\n",
    "\n",
    "# Cast 'registration_date' to DateType and 'customer_id' to IntegerType\n",
    "df_day1 = df_day1.withColumn(\n",
    "    \"registration_date\", F.to_date(\"registration_date\", \"M/d/yyyy H:mm\")\n",
    ").withColumn(\n",
    "    \"customer_id\", F.col(\"customer_id\").cast(IntegerType())\n",
    ")\n",
    "\n",
    "display(df_day1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b53b94cc-ed97-46dc-b60f-330d1fe9c500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load the existing customer dimension table\n",
    "SCD2customerdim = DeltaTable.forName(spark, \"retail.CustomerDim\")\n",
    "target_df = SCD2customerdim.toDF().cache()\n",
    "\n",
    "changed_records = df_day1.alias(\"src\").join(target_df.alias(\"tgt\"), \"customer_id\", \"left_outer\")\\\n",
    "    .filter(\n",
    "        (col(\"tgt.customer_id\").isNull()) |\n",
    "    (\n",
    "        (col(\"src.first_name\") != col(\"tgt.first_name\")) |\n",
    "        (col(\"src.last_name\") != col(\"tgt.last_name\")) |\n",
    "        (col(\"src.email\") != col(\"tgt.email\")) |\n",
    "        (col(\"src.gender\")!= col(\"tgt.gender\")) |\n",
    "        (col(\"src.address\") != col(\"tgt.address\")) |\n",
    "        (col(\"src.city\") != col(\"tgt.city\")) |\n",
    "        (col(\"src.state\") != col(\"tgt.state\")) |\n",
    "        (col(\"src.country\") != col(\"tgt.country\")) |\n",
    "        (col(\"src.zipcode\") != col(\"tgt.zipcode\")) |\n",
    "        (col(\"src.contact_no\") != col(\"tgt.contact_no\"))\n",
    "    )\n",
    "    ).select(\"src.*\")\n",
    "\n",
    "changed_records = changed_records.withColumn(\"Effective_start_date\", col(\"registration_date\").cast(\"date\"))\n",
    "\n",
    "max_sk = target_df.selectExpr(\"coalesce(max(customer_id), 0)\").first()[0]\n",
    "changed_records = changed_records.withColumn(\"Customer_Key\", row_number().over(Window.orderBy(\"customer_id\")) + max_sk)\n",
    "display(changed_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b48242e-b6dc-476b-8bf6-87cba4f4c4ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import lit, col, when\n",
    "\n",
    "changed_records = changed_records.withColumn(\"Effective_end_date\", lit(None).cast(\"date\")) \\\n",
    "    .withColumn(\"Is_active\", lit(\"Y\")) \\\n",
    "    .withColumn(\"status\", when(col(\"customer_id\").isin([c.customer_id for c in target_df.collect()]), lit(\"updated\")).otherwise(lit(\"New\")))\n",
    "\n",
    "display(changed_records)\n",
    "print(changed_records.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ff37e26-3895-4195-b9da-56d82102dc6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import coalesce, col, lit, expr\n",
    "\n",
    "changed_records = changed_records.select(\n",
    "    'customer_id', 'first_name', 'last_name', 'email', 'gender', 'address', \n",
    "    'city', 'state', 'country', 'zipcode', 'contact_no', 'registration_date', \n",
    "    'membership_status', 'Ingestion_time', 'Effective_start_date', \n",
    "    'Effective_end_date', 'Is_active', 'status', 'Customer_Key'\n",
    ")\n",
    "\n",
    "# ensuring customer key is non nullable\n",
    "changed_records = changed_records.withColumn(\"Customer_Key\", coalesce(col(\"Customer_Key\"), lit(0))).dropDuplicates(['customer_id'])\n",
    "\n",
    "# expire existing records\n",
    "SCD2customerdim.alias(\"tgt\").merge(changed_records.alias(\"src\"), \n",
    "    \"tgt.customer_id = src.customer_id and tgt.Is_active='Y'\")\\\n",
    "        .whenMatchedUpdate(\n",
    "            set = {\n",
    "                \"Effective_End_Dt\": col(\"src.Effective_start_date\") - expr(\"INTERVAL 1 DAY\"),\n",
    "                \"Is_active\": lit(\"N\"),\n",
    "                \"status\": lit(\"Expired\")\n",
    "            }\n",
    "        ).execute()\n",
    "\n",
    "new_inserts = changed_records.filter(col(\"status\") == \"New\").cache()\n",
    "updated_records = changed_records.filter(col(\"status\") == \"updated\").cache()\n",
    "\n",
    "print(\"Newly inserted records count = \", new_inserts.count())\n",
    "print(\"Updated records count = \", updated_records.count())\n",
    "\n",
    "if updated_records.count() > 0:\n",
    "    updated_records.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"retail.CustomerDim\")\n",
    "if new_inserts.count() > 0:\n",
    "    new_inserts.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(\"retail.CustomerDim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c04b422-97f4-4f83-86b7-d3a0aca1261b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from retail.CustomerDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5532ec01-0402-450c-96e9-81761a0542c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8947490234547972,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Retail_Gold_incrementalload",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
