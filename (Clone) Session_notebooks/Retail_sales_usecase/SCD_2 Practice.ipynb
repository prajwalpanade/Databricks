{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6741d04b-4c4f-4813-a13c-b2b04cebe08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary functions and types\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Define Schema and Initial Load\n",
    "base_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"email\", StringType()),\n",
    "    StructField(\"contact_no\", StringType())\n",
    "])\n",
    "\n",
    "# Day 0 Data\n",
    "data_day0 = [\n",
    "    (101, 'John', 'john@email.com', '9876543210'),\n",
    "    (102, 'Alice', 'alice@email.com', '9876541234'),\n",
    "    (103, 'Bob', 'bob@email.com', '8765432190')\n",
    "]\n",
    "\n",
    "# Convert to DataFrame with additional SCD2 columns (load_date, end_date, is_current, and status)\n",
    "df_day0 = spark.createDataFrame(data_day0, base_schema)\n",
    "display(df_day0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77504486-7772-404e-8c6f-355a7ceba77c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_day0 = df_day0.withColumn(\"load_date\", F.lit(\"2025-04-04\").cast(\"date\")) \\\n",
    "    .withColumn(\"end_date\", F.lit(None).cast(\"date\")) \\\n",
    "    .withColumn(\"is_current\", F.lit(1)) \\\n",
    "        \n",
    "display(df_day0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13321807-8d13-4cb5-b347-e03514873172",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_day0_with_surrogate = df_day0.withColumn(\"customer_key\", F.row_number().over(Window.orderBy(\"customer_id\")))\n",
    "df_day0_with_surrogate = df_day0_with_surrogate.drop(\"surrogate_key\")\n",
    "display(df_day0_with_surrogate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82999066-e682-47ea-a28b-81fb97d4fe89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the initial data to a Delta table\n",
    "df_day0_with_surrogate.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/Prajwal/Retail_sales_usecase/SCD2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03c59a67-7c6a-4eda-9e14-db0392a15596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the file from delta table\n",
    "df_day0_with_surrogate = spark.read.format(\"delta\").load(\"/mnt/Prajwal/Retail_sales_usecase/SCD2\")\n",
    "display(df_day0_with_surrogate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2fbf2ef-d792-4ca7-982b-8a1264b9430d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_day1 = [\n",
    "    (101, 'John', 'john.new@email.com', '9876543210'),  # Email changed\n",
    "    (102, 'Alice', 'alice@email.com', '9999999999'),    # Phone changed\n",
    "    (103, 'Bob', 'bob@email.com', '8765432190'),         # No change\n",
    "    (104, 'Mike', 'mike@email.com', '7777777777')        # New customer\n",
    "]\n",
    "\n",
    "df_day1 = spark.createDataFrame(source_day1, base_schema)\n",
    "\n",
    "display(df_day1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75fbe1a0-49a2-44fb-953b-860efe460613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SCD2Skeycustomer_dim = spark.read.format(\"delta\").load(\"/mnt/Prajwal/Retail_sales_usecase/SCD2\")\n",
    "\n",
    "# Step 1: Convert Load Date (current date for this example)\n",
    "# load_date = F.current_date()\n",
    "# Day 1 is 5th Apr 2025\n",
    "load_date = F.lit(\"2025-04-05\").cast(\"date\")\n",
    "\n",
    "\n",
    "target_df = SCD2Skeycustomer_dim.cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f76994-539b-4903-baef-ef0f478f187f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Identify new or changed records by joining on customer_id\n",
    "# Incremental is handled here, ensuring only new or modified records are considered\n",
    "changed_records = df_day1.alias(\"src\").join(\n",
    "    target_df.alias(\"tgt\").filter(\"is_current = 1\"),\n",
    "    \"customer_id\",\n",
    "    \"left_outer\"\n",
    ").filter(\n",
    "    (F.col(\"tgt.customer_id\").isNull()) |  # New records\n",
    "    (\n",
    "        (F.col(\"src.email\") != F.col(\"tgt.email\")) |\n",
    "        (F.col(\"src.contact_no\") != F.col(\"tgt.contact_no\")) |\n",
    "        (F.col(\"src.first_name\") != F.col(\"tgt.first_name\"))\n",
    "    )\n",
    ").select(\"src.*\")\n",
    "\n",
    "changed_records.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc6d12d-e230-4449-be17-5fe7839bd413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Add Surrogate Keys\n",
    "max_sk = target_df.selectExpr(\"COALESCE(MAX(customer_key), 0)\").first()[0]\n",
    "\n",
    "window = Window.orderBy(\"customer_id\")\n",
    "changed_records = changed_records.withColumn(\n",
    "    \"customer_key\",  # Add the surrogate key as customer_key\n",
    "    F.row_number().over(window) + max_sk\n",
    ")\n",
    "\n",
    "display(changed_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc079234-3680-48ca-ad6b-7f21c4d6691e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Step 5: Expire old (previous current) records by setting end_date and is_current=0\n",
    "query = f\"\"\"\n",
    "UPDATE SCD2Skeycustomer_dim\n",
    "SET end_date = DATE_SUB('{load_date}', 1),\n",
    "    is_current = 0,\n",
    "    status = 'OU - Old Updated Passive'\n",
    "WHERE customer_id IN (\n",
    "    SELECT customer_id\n",
    "    FROM changed_records\n",
    ") AND is_current = 1\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SCD_2 Practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
