{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b46a6d-0d2b-49f6-aedd-c0f16981a983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, count, row_number, lit, current_timestamp, coalesce, max\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6988ac-bb83-481d-9de4-73aa44cc2b18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"/mnt/mock_prajwal/Mock2/silver/\"\n",
    "gold_path = \"/mnt/mock_prajwal/Mock2/gold/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6bff5b1-108b-4f06-8606-5b8e4f92f615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(silver_path + \"FF_Customer_Details_Day1\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c91ede45-8096-4d1c-82b1-95a161a9ec96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9782181b-4dc7-40cd-b13a-0f47cd7dd4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_df = spark.read.format(\"delta\").load(silver_path + \"CustContact\")\n",
    "df_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df938412-85a5-4691-9dbe-e70ba61e915d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join df and df_df with left join\n",
    "joined_df = df.join(df_df, on=\"customer_id\", how=\"left\")\n",
    "\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fde3d1d-5ea4-4d04-9bbb-84942cc52251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_selected = joined_df.select(\n",
    "    col(\"customer_id\"),\n",
    "    col(\"customer_name\"),\n",
    "    col(\"customer_dob\"),\n",
    "    col(\"customer_marital_status\"),\n",
    "    col(\"gender\"),\n",
    "    col(\"email\"),\n",
    "    col(\"phone_number\"),\n",
    "    col(\"country\"),\n",
    "    col(\"region\"),\n",
    "    col(\"state\"),\n",
    "    col(\"city\"),\n",
    "    col(\"postal_code\")\n",
    ").dropDuplicates(['customer_id'])\n",
    "\n",
    "df_selected = df_selected.withColumn(\"customer_key\", row_number().over(Window.orderBy(\"customer_id\")))\n",
    "\n",
    "display(df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad43b857-584a-4698-bb83-ea0bc34a8861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_selected.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "419882ff-1e0c-4d6d-88c8-77620a746ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_dob\", DateType(), True),\n",
    "    StructField(\"customer_marital_status\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone_number\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"customer_key\", IntegerType(), False),\n",
    "    StructField(\"start_date\", TimestampType(), False),\n",
    "    StructField(\"end_date\", TimestampType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), False),\n",
    "    StructField(\"last_modified\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Create the schema if it does not exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS Prajwal_Mock\")\n",
    "\n",
    "# Create the table with the specified schema\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Prajwal_Mock.Dim_Customer (\n",
    "        customer_id INT,\n",
    "        customer_name STRING,\n",
    "        customer_dob DATE,\n",
    "        customer_marital_status STRING,\n",
    "        gender STRING,\n",
    "        email STRING,\n",
    "        phone_number STRING,\n",
    "        country STRING,\n",
    "        region STRING,\n",
    "        state STRING,\n",
    "        city STRING,\n",
    "        postal_code STRING,\n",
    "        customer_key INT NOT NULL,\n",
    "        start_date TIMESTAMP NOT NULL,\n",
    "        end_date TIMESTAMP,\n",
    "        is_active BOOLEAN NOT NULL,\n",
    "        last_modified TIMESTAMP NOT NULL\n",
    "    )\n",
    "        USING DELTA \n",
    "    LOCATION \"/mnt/mock_prajwal/Mock2/gold/Dim_Customer_test\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7919453c-d871-4980-ae22-c60fb35b2ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load target table\n",
    "target_df = spark.read.table(\"Prajwal_Mock.Dim_Customer\")\n",
    "\n",
    "# Check if 'customer_key' column exists in the target table\n",
    "if 'customer_key' in target_df.columns:\n",
    "    # Finding max of previous data from table\n",
    "    max_key = target_df.agg(coalesce(max(\"customer_key\"), lit(0))).collect()[0][0]\n",
    "else:\n",
    "    max_key = 0\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.orderBy(\"customer_id\")\n",
    "\n",
    "# Add row number and customer_key\n",
    "source_df_keyed = df_selected \\\n",
    "    .withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"customer_key\", col(\"rn\") + lit(max_key)) \\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "# Add audit columns to source\n",
    "source_df_audit_col = source_df_keyed \\\n",
    "    .withColumn(\"start_date\", current_timestamp()) \\\n",
    "    .withColumn(\"end_date\", lit(None).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"is_active\", lit(True)) \\\n",
    "    .withColumn(\"last_modified\", current_timestamp())\n",
    "\n",
    "# Convert target to DeltaTable\n",
    "target_table = DeltaTable.forName(spark, \"Prajwal_Mock.Dim_Customer\")\n",
    "\n",
    "# Merge condition (on business key and active rows)\n",
    "merge_condition = \"target.customer_id = source.customer_id AND target.is_active = true\"\n",
    "\n",
    "# Perform Expiration\n",
    "target_table.alias(\"target\") \\\n",
    "    .merge(source_df_audit_col.alias(\"source\"), merge_condition) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"target.customer_name != source.customer_name OR \"\n",
    "                  \"target.city != source.city OR \"\n",
    "                  \"target.customer_marital_status != source.customer_marital_status OR \"\n",
    "                  \"target.gender != source.gender OR \"\n",
    "                  \"target.customer_dob != source.customer_dob\",\n",
    "        set={\n",
    "            \"end_date\": current_timestamp(),\n",
    "            \"is_active\": lit(False),\n",
    "            \"last_modified\": current_timestamp()\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "# Insert records with new keys (new business keys that never existed)\n",
    "updated_target_df = spark.read.table(\"Prajwal_Mock.Dim_Customer\").filter(\"is_active=true\").select(\"customer_id\")\n",
    "insert_df = source_df_audit_col.join(updated_target_df, on=\"customer_id\", how=\"left_anti\")\n",
    "\n",
    "insert_df.write.format(\"delta\").mode(\"append\").save(\"/mnt/mock_prajwal/Mock2/gold/Dim_Customer_test\")\n",
    "\n",
    "# Ensure the schema matches the target table\n",
    "# insert_df = insert_df.select(*[col for col in target_df.columns if col in insert_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378adee3-8205-4242-80e3-99ebb7d2d356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(source_df_audit_col) # 3records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "719bf6b4-d9f2-4bc6-8b91-952d9d057733",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(updated_target_df) #1130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ada5c7-0042-4b96-97ae-1c7134680902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(insert_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f2d89b-972a-4361-8728-9b6188778320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(insert_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573431e9-4bcc-4cc5-8a13-7ee060263cac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from Prajwal_Mock.Dim_Customer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e9678e-db59-4429-ba50-98eef2db8384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.format(\"delta\").load(\"/mnt/mock_prajwal/Mock2/gold/Dim_Customer_test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e934fab-1100-4c30-a8f1-4644627f8298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.table(\"Prajwal_Mock.Dim_Customer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a035bf9e-b728-464a-bb24-23f9011dbfc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "insert_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/mock_prajwal/Mock2/gold/Dim_Customer\")\n",
    "display(insert_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b3fa14-2410-40ac-abaf-7a311b58f944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"/mnt/mock_prajwal/Mock2/gold/Dim_Customer\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e328dba2-b918-43b3-a125-033ee992f409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check updated rows\n",
    "updated_rows = target_table.toDF().filter(\"is_active = false AND end_date IS NOT NULL AND last_modified >= current_timestamp() - interval 1 day\")\n",
    "\n",
    "# Check inserted rows\n",
    "inserted_rows = insert_df\n",
    "\n",
    "# Count updated and inserted rows\n",
    "updated_count = updated_rows.count()\n",
    "inserted_count = inserted_rows.count()\n",
    "\n",
    "# Display updated and inserted rows along with their counts\n",
    "display(updated_rows)\n",
    "display(inserted_rows)\n",
    "display(spark.createDataFrame([(updated_count, inserted_count)], [\"Updated Rows Count\", \"Inserted Rows Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a38526-1d81-4489-a8af-fe9952e4c649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop the table if it exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS Prajwal_Mock.Dim_Customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559fd759-c792-4396-a495-107aae3147f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count records in the gold layer\n",
    "gold_count = spark.read.format(\"delta\").load(\"/mnt/mock_prajwal/Mock2/gold/Dim_Customer\").count()\n",
    "\n",
    "# Assuming silver layer is stored in DataFrames named silver_df_day0 and silver_df_day1\n",
    "silver_df_day0 = spark.read.format(\"delta\").load(silver_path + \"FF_Customer_Details_Day0\")\n",
    "silver_df_day1 = spark.read.format(\"delta\").load(silver_path + \"FF_Customer_Details_Day1\")\n",
    "\n",
    "silver_count_day0 = silver_df_day0.count()\n",
    "silver_count_day1 = silver_df_day1.count()\n",
    "\n",
    "# Display counts\n",
    "display(spark.createDataFrame([(silver_count_day0, silver_count_day1, gold_count)], [\"Silver Layer Day 0 Count\", \"Silver Layer Day 1 Count\", \"Gold Layer Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e112d13b-a6e8-4d98-bb54-9a316e20450d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_df = spark.read.format(\"delta\").load(\"/mnt/mock_prajwal/Mock2/gold/Dim_Customer\")\n",
    "display(gold_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Customer_dim_day1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
