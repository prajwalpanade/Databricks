{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b46a6d-0d2b-49f6-aedd-c0f16981a983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, count, row_number, lit, current_timestamp, coalesce, max\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d6988ac-bb83-481d-9de4-73aa44cc2b18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"/mnt/mock_prajwal/Mock2/silver/\"\n",
    "gold_path = \"/mnt/mock_prajwal/Mock2/gold/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6bff5b1-108b-4f06-8606-5b8e4f92f615",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(silver_path + \"FF_Customer_Details_Day0\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9782181b-4dc7-40cd-b13a-0f47cd7dd4b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_df = spark.read.format(\"delta\").load(silver_path + \"CustContact\")\n",
    "df_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0c56b8-67b2-487b-bb7c-982238443f5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select customer_id from both dataframes\n",
    "df_customer_ids = df.select(col(\"customer_id\")).dropDuplicates()\n",
    "df_df_customer_ids = df_df.select(col(\"customer_id\")).dropDuplicates()\n",
    "\n",
    "# Find orphan customer_ids in df\n",
    "orphans_in_df = df_customer_ids.join(df_df_customer_ids, on=\"customer_id\", how=\"left_anti\")\n",
    "\n",
    "# Find orphan customer_ids in df_df\n",
    "orphans_in_df_df = df_df_customer_ids.join(df_customer_ids, on=\"customer_id\", how=\"left_anti\")\n",
    "\n",
    "display(orphans_in_df)\n",
    "display(orphans_in_df_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df938412-85a5-4691-9dbe-e70ba61e915d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exclude orphan records\n",
    "non_orphans_in_df = df_customer_ids.join(df_df_customer_ids, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# Join both dataframes on non-orphan customer_ids\n",
    "df_non_orphans = df.join(non_orphans_in_df, on=\"customer_id\", how=\"inner\")\n",
    "df_df_non_orphans = df_df.join(non_orphans_in_df, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "# Join the two non-orphan dataframes\n",
    "joined_df = df_non_orphans.join(df_df_non_orphans, on=\"customer_id\", how=\"inner\")\n",
    "\n",
    "display(joined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fde3d1d-5ea4-4d04-9bbb-84942cc52251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_selected = joined_df.select(\n",
    "    col(\"customer_id\"),\n",
    "    col(\"customer_name\"),\n",
    "    col(\"customer_dob\"),\n",
    "    col(\"customer_marital_status\"),\n",
    "    col(\"gender\"),\n",
    "    col(\"email\"),\n",
    "    col(\"phone_number\"),\n",
    "    col(\"country\"),\n",
    "    col(\"region\"),\n",
    "    col(\"state\"),\n",
    "    col(\"city\"),\n",
    "    col(\"postal_code\")\n",
    ").dropDuplicates(['customer_id'])\n",
    "\n",
    "df_selected = df_selected.withColumn(\"customer_key\", row_number().over(Window.orderBy(\"customer_id\")))\n",
    "\n",
    "display(df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad43b857-584a-4698-bb83-ea0bc34a8861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_selected.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "419882ff-1e0c-4d6d-88c8-77620a746ce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"customer_dob\", DateType(), True),\n",
    "    StructField(\"customer_marital_status\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone_number\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"postal_code\", StringType(), True),\n",
    "    StructField(\"customer_key\", IntegerType(), False),\n",
    "    StructField(\"start_date\", TimestampType(), False),\n",
    "    StructField(\"end_date\", TimestampType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), False),\n",
    "    StructField(\"last_modified\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Create the schema if it does not exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS Prajwal_Mock\")\n",
    "\n",
    "# Create the table with the specified schema\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Prajwal_Mock.Dim_Customer (\n",
    "        customer_id INT,\n",
    "        customer_name STRING,\n",
    "        customer_dob DATE,\n",
    "        customer_marital_status STRING,\n",
    "        gender STRING,\n",
    "        email STRING,\n",
    "        phone_number STRING,\n",
    "        country STRING,\n",
    "        region STRING,\n",
    "        state STRING,\n",
    "        city STRING,\n",
    "        postal_code STRING,\n",
    "        customer_key INT NOT NULL,\n",
    "        start_date TIMESTAMP NOT NULL,\n",
    "        end_date TIMESTAMP,\n",
    "        is_active BOOLEAN NOT NULL,\n",
    "        last_modified TIMESTAMP NOT NULL\n",
    "    )\n",
    "        USING DELTA \n",
    "    LOCATION \"/mnt/mock_prajwal/Mock2/gold/Dim_Customer\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7919453c-d871-4980-ae22-c60fb35b2ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load target table\n",
    "target_df = spark.read.table(\"Prajwal_Mock.Dim_Customer\")\n",
    "\n",
    "# Check if 'customer_key' column exists in the target table\n",
    "if 'customer_key' in target_df.columns:\n",
    "    # Finding max of previous data from table\n",
    "    max_key = target_df.agg(coalesce(max(\"customer_key\"), lit(0))).collect()[0][0]\n",
    "else:\n",
    "    max_key = 0\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.orderBy(\"customer_id\")\n",
    "\n",
    "# Add row number and customer_key\n",
    "source_df_keyed = df_selected \\\n",
    "    .withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"customer_key\", col(\"rn\") + lit(max_key)) \\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "# Add audit columns to source\n",
    "source_df_audit_col = source_df_keyed \\\n",
    "    .withColumn(\"start_date\", current_timestamp()) \\\n",
    "    .withColumn(\"end_date\", lit(None).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"is_active\", lit(True)) \\\n",
    "    .withColumn(\"last_modified\", current_timestamp())\n",
    "\n",
    "# Convert target to DeltaTable\n",
    "target_table = DeltaTable.forName(spark, \"Prajwal_Mock.Dim_Customer\")\n",
    "\n",
    "# Merge condition (on business key and active rows)\n",
    "merge_condition = \"target.customer_id = source.customer_id AND target.is_active = true\"\n",
    "\n",
    "# Perform Expiration\n",
    "target_table.alias(\"target\") \\\n",
    "    .merge(source_df_audit_col.alias(\"source\"), merge_condition) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"target.customer_name != source.customer_name OR \"\n",
    "                  \"target.city != source.city OR \"\n",
    "                  \"target.customer_marital_status != source.customer_marital_status OR \"\n",
    "                  \"target.gender != source.gender OR \"\n",
    "                  \"target.customer_dob != source.customer_dob\",\n",
    "        set={\n",
    "            \"end_date\": current_timestamp(),\n",
    "            \"is_active\": lit(False),\n",
    "            \"last_modified\": current_timestamp()\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "# Insert records with new keys (new business keys that never existed)\n",
    "updated_target_df = spark.read.table(\"Prajwal_Mock.Dim_Customer\").filter(\"is_active=true\").select(\"customer_id\")\n",
    "insert_df = source_df_audit_col.join(updated_target_df, on=\"customer_id\", how=\"left_anti\")\n",
    "\n",
    "insert_df.write.format(\"delta\").mode(\"append\").save(\"/mnt/mock_prajwal/Mock2/gold/Dim_Customer\")\n",
    "\n",
    "# Ensure the schema matches the target table\n",
    "# insert_df = insert_df.select(*[col for col in target_df.columns if col in insert_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a38526-1d81-4489-a8af-fe9952e4c649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Drop the table if it exists\n",
    "# spark.sql(\"DROP TABLE IF EXISTS Prajwal_Mock.Dim_Customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "559fd759-c792-4396-a495-107aae3147f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count records in the gold layer\n",
    "gold_count = insert_df.count()\n",
    "\n",
    "# Assuming silver layer is stored in a DataFrame named silver_df\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path + \"FF_Customer_Details_Day0\")\n",
    "silver_count = silver_df.count()\n",
    "\n",
    "# Display counts\n",
    "display(spark.createDataFrame([(silver_count, gold_count)], [\"Silver Layer Count\", \"Gold Layer Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e112d13b-a6e8-4d98-bb54-9a316e20450d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_df = spark.read.format(\"delta\").load(\"/mnt/mock_prajwal/Mock2/gold/Dim_Customer\")\n",
    "display(gold_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Customer_dim",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
