{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bcabe54-4802-48be-b78c-e4e9a34ff57e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, count, row_number, lit, current_timestamp, coalesce, max\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "583cd885-15a2-429b-abce-9f1076313c95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"/mnt/mock_prajwal/Mock2/silver/\"\n",
    "gold_path = \"/mnt/mock_prajwal/Mock2/gold/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bfec95d-21ed-46dd-bde5-aae32d7f4dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(silver_path + \"FF_Customer_Details_Day0\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b7f2d2a-5a88-42c3-af39-4384cb5c2420",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_dim = df.select(\"product_name\", \"product_sub_category\", \"product_category\", \"product_container\", \"product_base_margin\").dropDuplicates(['product_name'])\n",
    "\n",
    "product_dim = product_dim.withColumn(\"product_key\", row_number().over(Window.orderBy(\"product_name\")))\n",
    "display(product_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0ea62f5-d2c6-48b8-8582-ae5c5191d01b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"product_sub_category\", StringType(), True),\n",
    "    StructField(\"product_category\", StringType(), True),\n",
    "    StructField(\"product_container\", StringType(), True),\n",
    "    StructField(\"product_base_margin\", DoubleType(), True),\n",
    "    StructField(\"product_key\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Create the schema if it does not exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS Prajwal_Mock\")\n",
    "\n",
    "# Create the table if it does not exist\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Prajwal_Mock.Dim_Product (\n",
    "        product_name STRING,\n",
    "        product_sub_category STRING,\n",
    "        product_category STRING,\n",
    "        product_container STRING,\n",
    "        product_base_margin DOUBLE,\n",
    "        product_key INTEGER NOT NULL\n",
    "    )\n",
    "    USING DELTA \n",
    "    LOCATION \"/mnt/mock_prajwal/Mock2/gold/Dim_Product\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27f808a7-dd6d-4b09-a774-f57df5e1239f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "product_dim.write.mode(\"overwrite\").format(\"delta\").save(gold_path + \"Dim_Product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339b3dd6-8443-409b-bb6a-7497d89089fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from gold path\n",
    "gold_path = \"/mnt/mock_prajwal/Mock2/gold/\"\n",
    "product_dim_df = spark.read.format(\"delta\").load(\"/mnt/mock_prajwal/Mock2/gold/Dim_Product\")\n",
    "\n",
    "# Read from product dim table\n",
    "product_dim_table_df = spark.table(\"Prajwal_Mock.Dim_Product\")\n",
    "\n",
    "# Display the dataframes\n",
    "display(product_dim_df)\n",
    "display(product_dim_table_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1f5f491-36a9-46b5-9b0d-cbd9abaae50b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count records in the gold layer\n",
    "gold_count = product_dim_df.count()\n",
    "\n",
    "# Assuming silver layer is stored in a DataFrame named silver_df\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path + \"FF_Customer_Details_Day0\")\n",
    "silver_count = silver_df.count()\n",
    "\n",
    "# Display counts\n",
    "display(spark.createDataFrame([(silver_count, gold_count)], [\"Silver Layer Count\", \"Gold Layer Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b289c9-1b3e-470b-8d87-41590b4c5499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Drop the product_dim table\n",
    "# spark.sql(\"DROP TABLE IF EXISTS Prajwal_Mock.Product_dim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8077d3f-712f-4d2c-97d9-7efb7452951d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show all tables from Prajwal_Mock\n",
    "tables_df = spark.sql(\"SHOW TABLES IN Prajwal_Mock\")\n",
    "display(tables_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Product_dim",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
