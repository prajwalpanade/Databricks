{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28bbdbc1-1284-4436-93a1-a0376530e62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "silver zone is responsible for cleaning, standardization, and applying business rules.\n",
    "Auto Optimize & Small File Compaction (Delta Optimization)\n",
    "Memory & Shuffle Optimizations (if transformations occur)\n",
    "Data Deduplication (Removing Duplicates)\n",
    "Adjusting Shuffle Partitions & Enabling AQE\n",
    "Data Type Optimization (Ensuring efficient storage & performance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea99460d-4a80-4d41-ab04-9de3b5af2070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Auto Optimize & Compact Small Files\n",
    "\n",
    "Enables Delta Lake's Auto Optimize whereever necessary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91d8f7a7-a7bd-4ef2-97d3-a6963f112e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SET spark.databricks.delta.optimizeWrite.enabled = true\")\n",
    "spark.sql(\"SET spark.databricks.delta.autoCompact.enabled = true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5a7999c-55bb-4f70-a74e-fd55f3836c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ce0fc1b-6f50-464a-8ca7-1fe71e59b017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../logs/logs_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c267997-f31a-4566-af18-cc2c94a8b02b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utilities/Futuredate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ea3da0-ea94-4ba2-ad6b-4e49b6e157aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utilities/Pastdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c62a3d-a7f2-477b-b213-ea0cf75205cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define paths for bronze and silver layers\n",
    "bronze_path = \"/mnt/mock_prajwal/Mock2/bronze/\"\n",
    "silver_path = \"/mnt/mock_prajwal/Mock2/silver/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76f378a4-0f30-4f20-8dd7-9ca47df79861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# try block to handle exceptions\n",
    "try:\n",
    "    # check if the bronze path exists\n",
    "    if dbutils.fs.ls(bronze_path):\n",
    "        # get the list of files in the bronze path\n",
    "        files = [file.name for file in dbutils.fs.ls(bronze_path)]\n",
    "        \n",
    "        # check if 'FF_Customer_Details_Day0/' exists in the files\n",
    "        if 'FF_Customer_Details_Day0/' in files:\n",
    "            # set file path and other file details\n",
    "            file_path = bronze_path + \"FF_Customer_Details_Day0\"\n",
    "            file_name = os.path.basename(file_path)\n",
    "            file_extension = file_name.split(\".\")[-1]\n",
    "            file_name = file_name.split(\".\")[0]\n",
    "            file_info = dbutils.fs.ls(file_path)[0]\n",
    "            file_size_kb = file_info.size / 1024\n",
    "            file_mod_time = datetime.fromtimestamp(file_info.modificationTime / 1000)\n",
    "            processed_by = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "            file_type = \"parquet\"  # Define file_type\n",
    "            Layer = \"silver\"\n",
    "            \n",
    "            # start time of the files\n",
    "            start_time = time.time()\n",
    "            log_message(file_path, file_type, file_size_kb, file_mod_time, None, \"PROCESSING\", 0, processed_by, f\"Reading Parquet File {file_name}\", Layer)\n",
    "            \n",
    "            # Load the parquet file into a DataFrame\n",
    "            df = spark.read.format(\"parquet\").load(bronze_path + \"FF_Customer_Details_Day0\")\n",
    "            \n",
    "            # Rename columns for consistency and readability\n",
    "            df_renamed = df.withColumnRenamed(\"Row ID\", \"row_id\") \\\n",
    "                .withColumnRenamed(\"Order Priority\", \"order_priority\") \\\n",
    "                .withColumnRenamed(\"Discount\", \"discount\") \\\n",
    "                .withColumnRenamed(\"Unit Price\", \"unit_price\") \\\n",
    "                .withColumnRenamed(\"Shipping Cost\", \"shipping_cost\") \\\n",
    "                .withColumnRenamed(\"Customer ID\", \"customer_id\") \\\n",
    "                .withColumnRenamed(\"Customer Name\", \"customer_name\") \\\n",
    "                .withColumnRenamed(\"Ship Mode\", \"ship_mode\") \\\n",
    "                .withColumnRenamed(\"Customer Segment\", \"customer_segment\") \\\n",
    "                .withColumnRenamed(\"Product Category\", \"product_category\") \\\n",
    "                .withColumnRenamed(\"Product Sub-Category\", \"product_sub_category\") \\\n",
    "                .withColumnRenamed(\"Product Container\", \"product_container\") \\\n",
    "                .withColumnRenamed(\"Product Name\", \"product_name\") \\\n",
    "                .withColumnRenamed(\"Product Base Margin\", \"product_base_margin\") \\\n",
    "                .withColumnRenamed(\"Country\", \"country\") \\\n",
    "                .withColumnRenamed(\"Region\", \"region\") \\\n",
    "                .withColumnRenamed(\"State or Province\", \"state\") \\\n",
    "                .withColumnRenamed(\"City\", \"city\") \\\n",
    "                .withColumnRenamed(\"Postal Code\", \"postal_code\") \\\n",
    "                .withColumnRenamed(\"Order Date\", \"order_date\") \\\n",
    "                .withColumnRenamed(\"Ship Date\", \"ship_date\") \\\n",
    "                .withColumnRenamed(\"Profit\", \"profit\") \\\n",
    "                .withColumnRenamed(\"Quantity ordered new\", \"quantity_ordered_new\") \\\n",
    "                .withColumnRenamed(\"Sales\", \"sales\") \\\n",
    "                .withColumnRenamed(\"Order ID\", \"order_id\") \\\n",
    "                .withColumnRenamed(\"Customer_DOB\", \"customer_dob\") \\\n",
    "                .withColumnRenamed(\"Customer_Maritial_Status\", \"customer_marital_status\") \\\n",
    "                .withColumnRenamed(\"Gender\", \"gender\") \\\n",
    "                .withColumnRenamed(\"Valid_From\", \"valid_from\") \\\n",
    "                .withColumnRenamed(\"Valid_To\", \"valid_to\") \\\n",
    "                .withColumnRenamed(\"Sales_Rep_Id\", \"sales_rep_id\") \\\n",
    "                .withColumnRenamed(\"Sales_Rep_Name\", \"sales_rep_name\") \\\n",
    "                .withColumnRenamed(\"ingestion_time\", \"ingestion_time\")\n",
    "            \n",
    "            # Apply UDF to filter past and future records\n",
    "            futuredate = udf(date_format_udf_Policy_future, DateType())\n",
    "            df_renamed = df_renamed.withColumn(\"ship_date\", futuredate(col(\"ship_date\")))\n",
    "            df_renamed = df_renamed.withColumn(\"valid_from\", futuredate(col(\"valid_from\")))\n",
    "            futuredate = udf(date_format_udf_Policy_future, DateType())\n",
    "            df_renamed = df_renamed.withColumn(\"order_date\", coalesce(futuredate(col(\"order_date\")), lit(\"2017-01-01\")))\n",
    "            df_re = df_renamed.withColumn(\"valid_to\", coalesce(col(\"valid_to\"), lit(\"2018-12-31\")))\n",
    "            \n",
    "            # Update priority column based on the given conditions\n",
    "            df_re = df_re.withColumn(\"order_priority\", \n",
    "                                     when(col(\"order_priority\") == \"High\", \"High\")\n",
    "                                     .when(col(\"order_priority\") == \"Low\", \"Low\")\n",
    "                                     .when(col(\"order_priority\") == \"Critical\", \"Critical\")\n",
    "                                     .when(col(\"order_priority\").isNull(), \"Not Specified\")\n",
    "                                     .when(col(\"order_priority\") == \"Medium\", \"Medium\")\n",
    "                                     .otherwise(col(\"order_priority\")))\n",
    "            \n",
    "            # Set default values for certain columns\n",
    "            df_re = df_re.withColumn(\"product_base_margin\", when(col(\"product_base_margin\").isNull(), 0.11).otherwise(col(\"product_base_margin\")))\n",
    "            \n",
    "            # Standardize ZIP codes (Ensuring 5-digit numeric values)\n",
    "            df_re = df_re.withColumn(\"postal_code\", regexp_replace(col(\"postal_code\"), \"[^0-9]\", \"\"))\n",
    "            df_re = df_re.withColumn(\"postal_code\", lpad(col(\"postal_code\"), 5, \"0\"))\n",
    "            \n",
    "            # Record count and processing time\n",
    "            record_count = df_re.count()\n",
    "            processing_time_sec = int(time.time() - start_time)\n",
    "            \n",
    "            # Final status\n",
    "            log_message(file_path, file_type, file_size_kb, file_mod_time, record_count, \"COMPLETED\", processing_time_sec, processed_by, f\"Successfully processed {file_path}\", Layer)\n",
    "            df_re.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"ingestion_time\").option(\"overwriteSchema\", \"true\").save(silver_path + \"FF_Customer_Details_Day0\")\n",
    "        \n",
    "        # check if 'CustContact/' exists in the files\n",
    "        if 'CustContact/' in files:\n",
    "            # set file path and other file details\n",
    "            file_path = bronze_path + \"CustContact\"\n",
    "            file_name = os.path.basename(file_path)\n",
    "            file_extension = file_name.split(\".\")[-1]\n",
    "            file_name = file_name.split(\".\")[0]\n",
    "            file_info = dbutils.fs.ls(file_path)[0]\n",
    "            file_size_kb = file_info.size / 1024\n",
    "            file_mod_time = datetime.fromtimestamp(file_info.modificationTime / 1000)\n",
    "            processed_by = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "            file_type = \"parquet\"  # Define file_type\n",
    "            Layer = \"silver\"\n",
    "            \n",
    "            # start time of the files\n",
    "            start_time = time.time()\n",
    "            log_message(file_path, file_type, file_size_kb, file_mod_time, None, \"PROCESSING\", 0, processed_by, f\"Reading Json and txt File {file_name}\", Layer)\n",
    "            \n",
    "            # Load the parquet file into a DataFrame\n",
    "            df = spark.read.format(\"parquet\").load(bronze_path + \"CustContact\")\n",
    "            \n",
    "            # Rename columns for consistency and readability\n",
    "            df = df.withColumnRenamed(\"Customer_ID\", \"customer_id\") \\\n",
    "                .withColumnRenamed(\"Email\", \"email\") \\\n",
    "                .withColumnRenamed(\"Phone_Number\", \"phone_number\") \\\n",
    "                .withColumnRenamed(\"ingestion_time\", \"ingestion_time\")\n",
    "            \n",
    "            # Ensure email is correctly formatted\n",
    "            df = df.withColumn('email', regexp_replace('email', r'[^a-zA-Z0-9@._-]', ''))\n",
    "            \n",
    "            # Standardize phone numbers (Ensuring numeric values)\n",
    "            df = df.withColumn(\"phone_number\", regexp_replace(col(\"phone_number\"), \"[^0-9]\", \"\"))\n",
    "            \n",
    "            # Record count and processing time\n",
    "            record_count = df.count()\n",
    "            processing_time_sec = int(time.time() - start_time)\n",
    "            \n",
    "            # Final status\n",
    "            log_message(file_path, file_type, file_size_kb, file_mod_time, record_count, \"COMPLETED\", processing_time_sec, processed_by, f\"Successfully processed {file_path}\", Layer)\n",
    "            df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"ingestion_time\").option(\"overwriteSchema\", \"true\").save(silver_path + \"CustContact\")\n",
    "# handle exceptions\n",
    "except Exception as e:\n",
    "    processing_time_sec = int(time.time() - start_time)\n",
    "    log_message(file_path, file_type, file_size_kb, file_mod_time, 0, \"FAILED\", processing_time_sec, processed_by, f\"Error processing file {file_path}: {str(e)}\", Layer)\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df0e2ff1-8aa3-4c84-82f2-1c044a557ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_path = \"/mnt/mock_prajwal/Mock2/logs\"\n",
    "df_logs = spark.read.format(\"delta\").load(log_path)\n",
    "display(df_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49860ecf-0eab-40fc-8bf9-46fd78a7de8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "silver_path = \"/mnt/mock_prajwal/Mock2/silver/\"\n",
    "\n",
    "df_customer_details = spark.read.format(\"delta\").load(silver_path + \"FF_Customer_Details_Day0\")\n",
    "df_cust_contact = spark.read.format(\"delta\").load(silver_path + \"CustContact\")\n",
    "\n",
    "display(df_customer_details)\n",
    "display(df_cust_contact)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver_All",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
