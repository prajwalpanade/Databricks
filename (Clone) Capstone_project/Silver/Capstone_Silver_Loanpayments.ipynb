{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94b35bc9-552d-4dd9-9ede-a1dd36f2a4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "silver zone is responsible for cleaning, standardization, and applying business rules.\n",
    "Auto Optimize & Small File Compaction (Delta Optimization)\n",
    "Memory & Shuffle Optimizations (if transformations occur)\n",
    "Data Deduplication (Removing Duplicates)\n",
    "Data Type Optimization (Ensuring efficient storage & performance)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0c9c367-4f18-4e45-8386-f5c3b03fd1f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Importing necessary libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import substring\n",
    "from pyspark.sql.functions import trim\n",
    "\n",
    "# Define paths for bronze and silver zones\n",
    "bronze_path = \"/mnt/Prajwal/Capstone_Project/bronze/Loanpayments\"\n",
    "silver_path = \"/mnt/Prajwal/Capstone_Project/silver/Loanpayments\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Loan_ID\", StringType(), True),\n",
    "    StructField(\"Payment_ID\", StringType(), True),\n",
    "    StructField(\"Payment_Amount\", StringType(), True),\n",
    "    StructField(\"Payment_Date\", StringType(), True),\n",
    "    StructField(\"Payment_Status\", StringType(), True),\n",
    "    StructField(\"Balance_Amount\", StringType(), True),\n",
    "    StructField(\"Penalty_Amount\", StringType(), True),\n",
    "    StructField(\"ingest_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(schema).option(\"header\", \"False\").load(\"/mnt/Prajwal/Capstone_Project/Source_Files/Loanpayments.csv\")\n",
    "\n",
    "\n",
    "df = df.withColumn(\"ingest_time\", lit(\"2025-05-02 09:56:01\").cast(TimestampType()))\n",
    "\n",
    "df = df.select([trim(col(c)).alias(c) for c in df.columns])\n",
    "\n",
    "df = df.withColumn(\"Payment_Amount\", col(\"Payment_Amount\").cast(DoubleType())) \\\n",
    "       .withColumn(\"Balance_Amount\", col(\"Balance_Amount\").cast(DoubleType())) \\\n",
    "       .withColumn(\"Penalty_Amount\", col(\"Penalty_Amount\").cast(DoubleType()))\n",
    "\n",
    "df = df.dropDuplicates([\"loan_id\"]).dropna()\n",
    "\n",
    "\n",
    "df = df.withColumn(\"Payment_Date\",\n",
    "                         when(to_date(col(\"payment_date\"), \"M/d/yyyy\").isNotNull(), to_date(col(\"payment_date\"), \"M/d/yyyy\"))\n",
    "                         .when(to_date(col(\"payment_date\"), \"d-MMM-yy\").isNotNull(), to_date(col(\"payment_date\"), \"d-MMM-yy\"))\n",
    "                         .otherwise(None))\n",
    "\n",
    "df = df.withColumn(\"Payment_Status\",  (trim(upper(col(\"Payment_Status\"))).alias(\"Payment_Status\")))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef26dbeb-6295-45e9-9b54-921b033bae43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").format(\"delta\").save(silver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3149a9dd-f015-422a-8051-1b3ef3e1deff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from the silver zone\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "display(silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea6b722d-9c77-4948-bac3-b659b2903564",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date, lit\n",
    "\n",
    "# Define the new data to be merged\n",
    "new_data_df = ...\n",
    "\n",
    "# Add effective date columns to the new data\n",
    "new_data_df = new_data_df.withColumn(\"effective_date\", current_date()).withColumn(\"end_date\", lit(None))\n",
    "\n",
    "# Read the existing data from the silver zone\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# Merge the new data with the existing data to implement SCD Type 2\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, silver_path)\n",
    "\n",
    "# Define the merge condition\n",
    "merge_condition = \"silver_df.id = new_data_df.id AND silver_df.end_date IS NULL\"\n",
    "\n",
    "# Perform the merge\n",
    "delta_table.alias(\"silver_df\").merge(\n",
    "    new_data_df.alias(\"new_data_df\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdate(\n",
    "    set={\"end_date\": current_date()}\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Display the updated silver zone data\n",
    "updated_silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "display(updated_silver_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Capstone_Silver_Loanpayments",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
