{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e43abac-6771-4d32-98ab-d10e24575c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "silver zone is responsible for cleaning, standardization, and applying business rules.\n",
    "Auto Optimize & Small File Compaction (Delta Optimization)\n",
    "Memory & Shuffle Optimizations (if transformations occur)\n",
    "Data Deduplication (Removing Duplicates)\n",
    "Data Type Optimization (Ensuring efficient storage & performance)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40373d90-a1ad-49ee-81d3-c6a23512e4b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import substring\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "# Define paths for bronze and silver zones\n",
    "bronze_path_day0 = \"/mnt/Prajwal/Capstone_Project/bronze/bankcustomer_source1_day0\"\n",
    "bronze_path_day1 = \"/mnt/Prajwal/Capstone_Project/bronze/bankcustomer_day1\"\n",
    "silver_path = \"/mnt/Prajwal/Capstone_Project/silver/bankcustomer_source\"\n",
    "\n",
    "# Check if day1 file exists in the bronze layer\n",
    "if dbutils.fs.ls(\"/mnt/Prajwal/Capstone_Project/bronze/\"):\n",
    "    files = [file.name for file in dbutils.fs.ls(\"/mnt/Prajwal/Capstone_Project/bronze/\")]\n",
    "    if \"bankcustomer_day11/\" in files:\n",
    "        # Implement SCD Type 2 logic\n",
    "        from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "        # Read existing data from the silver path if it exists\n",
    "        try:\n",
    "            df_existing = spark.read.format(\"delta\").load(silver_path)\n",
    "        except:\n",
    "            df_existing = spark.createDataFrame([], df.schema)\n",
    "\n",
    "        # Read data from the bronze path (day1)\n",
    "        df_new = spark.read.format(\"parquet\").load(bronze_path_day1)\n",
    "\n",
    "        # Trim spaces from column names\n",
    "        df_new = df_new.toDF(*[c.strip() for c in df_new.columns])\n",
    "\n",
    "        # Rename columns for consistency and readability\n",
    "        df_new = df_new.withColumnRenamed(\"CustomerID\", \"customer_id\") \\\n",
    "               .withColumnRenamed(\"CustomerName\", \"name\") \\\n",
    "               .withColumnRenamed(\"City\", \"city\") \\\n",
    "               .withColumnRenamed(\"PhoneNo\", \"phone_no\") \\\n",
    "               .withColumnRenamed(\"MaritalStatus\", \"maritial_status\") \\\n",
    "               .withColumnRenamed(\"Gender\", \"gender\") \\\n",
    "               .withColumnRenamed(\"EmailAddress\", \"email\")\n",
    "        \n",
    "        # Fill null values with default values\n",
    "        df_new = df_new.fillna({\n",
    "            'name': 'Unknown',\n",
    "            'city': 'Unknown',\n",
    "            'phone_no': '000-000-0000',\n",
    "            'maritial_status': 'Unknown',\n",
    "            'gender': 'Unknown',\n",
    "            'email': 'noemail@example.com'\n",
    "        })\n",
    "\n",
    "        # Extract area code from phone number\n",
    "        df_newdf = df_new.withColumn(\"area_code\", regexp_extract(col(\"phone_no\"), r\"(\\d{3})\", 1))\n",
    "\n",
    "        # Ensure email is correctly formatted\n",
    "        df_new = df_new.withColumn('email', regexp_replace('email', r'[^a-zA-Z0-9@._-]', ''))\n",
    "\n",
    "        # Change ingestion time to the format of yyyy-mm-dd hh:mm:ss\n",
    "        df_new = df_new.withColumn('ingestion_time', date_format(col('ingestion_time'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "        # Drop duplicate records based on customer_id\n",
    "        df_new = df_new.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "        # Drop the original ingestion time column\n",
    "        df_new = df_new.drop(\"ingest_time\")\n",
    "\n",
    "        # Define a UDF to fix date of birth (DOB) format\n",
    "        from pyspark.sql.functions import udf\n",
    "        from datetime import datetime\n",
    "\n",
    "        # def fix_dob(dob_str):\n",
    "        #     try:\n",
    "        #         dob = datetime.strptime(dob_str, '%d-%b-%y')\n",
    "        #         if dob.year > datetime.today().year:\n",
    "        #             dob = dob.replace(year=dob.year - 100)\n",
    "        #         return dob\n",
    "        #     except:\n",
    "        #         return None\n",
    "\n",
    "        #fix_dob_udf = udf(fix_dob, DateType())\n",
    "\n",
    "        # Apply the UDF to fix DOB and calculate age\n",
    "        # Apply the UDF to fix DOB and calculate age\n",
    "        df_new = df_new.withColumn(\"age\", floor(datediff(current_date(), col(\"DOB\")) / 365.25))\n",
    "\n",
    "        df_new = df_new.withColumn(\"ingestion_time_formatted\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "            # Add SCD Type 2 columns to the new data\n",
    "        df_new = df_new.withColumn(\"is_current\", lit(True)) \\\n",
    "                       .withColumn(\"start_date\", current_timestamp()) \\\n",
    "                       .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "        # Join new data with existing data to identify changes\n",
    "        df_joined = df_new.join(df_existing, \"customer_id\", \"left\")\n",
    "\n",
    "        # Identify records that have changed\n",
    "        df_changed = df_joined.filter(\n",
    "            (df_new[\"name\"] != df_existing[\"name\"]) |\n",
    "            (df_new[\"city\"] != df_existing[\"city\"]) |\n",
    "            (df_new[\"phone_no\"] != df_existing[\"phone_no\"]) |\n",
    "            (df_new[\"maritial_status\"] != df_existing[\"maritial_status\"]) |\n",
    "            (df_new[\"gender\"] != df_existing[\"gender\"]) |\n",
    "            (df_new[\"email\"] != df_existing[\"email\"]) |\n",
    "            (df_new[\"DOB\"] != df_existing[\"DOB\"]) |\n",
    "            (df_new[\"age\"] != df_existing[\"age\"]) |\n",
    "            (df_new[\"ingestion_time\"] != df_existing[\"ingestion_time\"]) |\n",
    "            (df_new[\"ingestion_time_formatted\"] != df_existing[\"ingestion_time_formatted\"])\n",
    "        ).select(df_new[\"*\"])\n",
    "\n",
    "        # Mark existing records as not current\n",
    "        df_existing_updated = df_existing.join(df_changed, \"customer_id\", \"left_anti\") \\\n",
    "                                         .withColumn(\"is_current\", lit(False)) \\\n",
    "                                         .withColumn(\"end_date\", current_timestamp())\n",
    "\n",
    "        # Create a DeltaTable object for the existing data\n",
    "        deltaTable = DeltaTable.forPath(spark, silver_path)\n",
    "\n",
    "       # Merge new data with existing data to identify changes and insert new records\n",
    "        df_final = deltaTable.alias(\"existing\").merge(\n",
    "                df_new.alias(\"new\"),\n",
    "                \"existing.customer_id = new.customer_id\"\n",
    "        ).whenMatchedUpdate(\n",
    "                        condition=\"existing.name != new.name OR \"\n",
    "                        \"existing.city != new.city OR \"\n",
    "                        \"existing.phone_no != new.phone_no OR \"\n",
    "                        \"existing.maritial_status != new.maritial_status OR \"\n",
    "                        \"existing.gender != new.gender OR \"\n",
    "                        \"existing.DOB != new.DOB OR \"\n",
    "                        \"existing.email != new.email\",\n",
    "            set={\n",
    "                \"is_current\": lit(False),\n",
    "                \"end_date\": current_timestamp(),\n",
    "                 }\n",
    "        ).execute()\n",
    "\n",
    "        #Insert records with new keys (new business keys that never existed)\n",
    "        updated_target_df = deltaTable.toDF().filter(\"is_current = true\").select(\"customer_id\")\n",
    "        insert_df = df_new.join(updated_target_df, on=\"customer_id\", how=\"left_anti\")\n",
    "\n",
    "        #final_insert = insert_df.withColumn('is_current', lit(True) \\\n",
    "         #                               .withColumn(\"start_date\", current_timestamp()) \\\n",
    "          #                              .withColumn(\"end_date\", lit(None).cast(\"timestamp\")))\n",
    "\n",
    "        print(\"data is updated and inserted in silver path\")\n",
    "\n",
    "        # Write the final DataFrame to the silver path\n",
    "        insert_df.write.mode(\"append\").format(\"delta\").partitionBy(\"ingestion_time_formatted\").option(\"mergeSchema\", \"true\").save(silver_path)\n",
    "\n",
    "        # Count updated and newly inserted records\n",
    "        updated_count = df_changed.count()\n",
    "        newly_inserted_count = df_new.count() # #updated_count\n",
    "\n",
    "        # Display the counts\n",
    "        print(f\"Updated record count: {updated_count}\")\n",
    "        print(f\"Newly inserted record count: {newly_inserted_count}\")\n",
    "    else:\n",
    "        # Read data from the bronze path (day0)\n",
    "        df = spark.read.format(\"parquet\").option(\"inferSchema\", \"true\").load(bronze_path_day0)\n",
    "\n",
    "        # Drop the last 2 rows of the DataFrame\n",
    "        df = df.limit(df.count() - 2)\n",
    "\n",
    "        # Trim spaces from column names\n",
    "        df = df.toDF(*[c.strip() for c in df.columns])\n",
    "\n",
    "        # Rename columns for consistency and readability\n",
    "        df = df.withColumnRenamed(\"_Customerid\", \"customer_id\") \\\n",
    "               .withColumnRenamed(\"C1ustomer Name\", \"name\") \\\n",
    "               .withColumnRenamed(\"City\", \"city\") \\\n",
    "               .withColumnRenamed(\"Phoneno\", \"phone_no\") \\\n",
    "               .withColumnRenamed(\"Maritial_Status\", \"maritial_status\") \\\n",
    "               .withColumnRenamed(\"Gender\", \"gender\") \\\n",
    "               .withColumnRenamed(\"EmailAddress\", \"email\")\n",
    "\n",
    "        # Fill null values with default values\n",
    "        df = df.fillna({\n",
    "            'name': 'Unknown',\n",
    "            'city': 'Unknown',\n",
    "            'phone_no': '000-000-0000',\n",
    "            'maritial_status': 'Unknown',\n",
    "            'gender': 'Unknown',\n",
    "            'email': 'noemail@example.com'\n",
    "        })\n",
    "\n",
    "        # Extract area code from phone number\n",
    "        df = df.withColumn(\"area_code\", regexp_extract(col(\"phone_no\"), r\"(\\d{3})\", 1))\n",
    "\n",
    "        # Ensure email is correctly formatted\n",
    "        df = df.withColumn('email', regexp_replace('email', r'[^a-zA-Z0-9@._-]', ''))\n",
    "\n",
    "        # Change ingestion time to the format of yyyy-mm-dd hh:mm:ss\n",
    "        df = df.withColumn('ingestion_time', date_format(col('ingest_time'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "        # Drop duplicate records based on customer_id\n",
    "        df = df.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "        # Drop the original ingestion time column\n",
    "        df = df.drop(\"ingest_time\")\n",
    "\n",
    "        # Define a UDF to fix date of birth (DOB) format\n",
    "        from pyspark.sql.functions import udf\n",
    "        from datetime import datetime\n",
    "\n",
    "        def fix_dob(dob_str):\n",
    "            try:\n",
    "                dob = datetime.strptime(dob_str, '%d-%b-%y')\n",
    "                if dob.year > datetime.today().year:\n",
    "                    dob = dob.replace(year=dob.year - 100)\n",
    "                return dob\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "        fix_dob_udf = udf(fix_dob, DateType())\n",
    "\n",
    "        # Apply the UDF to fix DOB and calculate age\n",
    "        df = df.withColumn(\"DOB\", fix_dob_udf(col(\"DOB\")))\n",
    "        df_src1 = df.withColumn(\"age\", floor(datediff(current_date(), col(\"DOB\")) / 365.25))\n",
    "\n",
    "        # Define paths for bronze source2 \n",
    "        bronze_path = \"/mnt/Prajwal/Capstone_Project/bronze/bankcustomer_source2_day0\"\n",
    "\n",
    "        # Read data from the bronze path\n",
    "        df_src2 = spark.read.format(\"parquet\").load(bronze_path)\n",
    "\n",
    "        # Rename columns for consistency and readability\n",
    "        df_src2 = df_src2.withColumnRenamed(\"CustomerID\", \"customer_id\") \\\n",
    "               .withColumnRenamed(\"CustomerName\", \"name\") \\\n",
    "               .withColumnRenamed(\"City\", \"city\") \\\n",
    "               .withColumnRenamed(\"Phoneno\", \"phone_no\") \\\n",
    "               .withColumnRenamed(\"MaritalStatus\", \"maritial_status\") \\\n",
    "               .withColumnRenamed(\"Gender\", \"gender\") \\\n",
    "               .withColumnRenamed(\"EmailAddress\", \"email\")\n",
    "\n",
    "        # Trim spaces from column names\n",
    "        df_src2 = df_src2.toDF(*[c.strip() for c in df_src2.columns])\n",
    "\n",
    "        # Extract area code from phone number\n",
    "        df_src2 = df_src2.withColumn(\"area_code\", regexp_extract(col(\"phone_no\"), r\"^(\\d{3})\", 1))\n",
    "\n",
    "        # Ensure email is correctly formatted\n",
    "        df_src2 = df_src2.withColumn('email', regexp_replace('email', r'[^a-zA-Z0-9@._-]', ''))\n",
    "\n",
    "        # Change ingestion time to the format of yyyy-mm-dd hh:mm:ss\n",
    "        df_src2 = df_src2.withColumn('ingestion_time', date_format(col('ingest_time'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "        # Drop the original ingestion time column\n",
    "        df_src2 = df_src2.drop(\"ingest_time\")\n",
    "\n",
    "        # Drop duplicate records based on customer_id\n",
    "        df_src2 = df_src2.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "        # Cast DOB to proper format\n",
    "        df_src2 = df_src2.withColumn(\"DOB\", to_date(col(\"DOB\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "        # Calculate age\n",
    "        df_src2 = df_src2.withColumn(\"age\", floor(datediff(current_date(), col(\"DOB\")) / 365.25))\n",
    "\n",
    "        df_src2 = df_src2.select('customer_id',\n",
    "         'name',\n",
    "         'city',\n",
    "         'phone_no',\n",
    "         'maritial_status',\n",
    "         'gender',\n",
    "         'DOB',\n",
    "         'email',\n",
    "         'area_code',\n",
    "         'ingestion_time',\n",
    "         'age')\n",
    "\n",
    "        df = df_src1.union(df_src2)\n",
    "\n",
    "        df = df.withColumn(\"ingestion_time_formatted\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "        # Implement SCD Type 2 logic for day0 data\n",
    "        from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "        # Read existing data from the silver path if it exists\n",
    "        try:\n",
    "            df_existing = spark.read.format(\"delta\").load(silver_path)\n",
    "        except:\n",
    "            df_existing = spark.createDataFrame([], df.schema)\n",
    "\n",
    "        # Add SCD Type 2 columns to the new data\n",
    "        df_new = df.withColumn(\"is_current\", lit(True)) \\\n",
    "                   .withColumn(\"start_date\", current_timestamp()) \\\n",
    "                   .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "        # Create a DeltaTable object for the existing data\n",
    "        deltaTable = DeltaTable.forPath(spark, silver_path)\n",
    "\n",
    "        # Merge new data with existing data to identify changes and insert new records\n",
    "        df_final = deltaTable.alias(\"existing\").merge(\n",
    "                df_new.alias(\"new\"),\n",
    "                \"existing.customer_id = new.customer_id\"\n",
    "        ).whenMatchedUpdate(\n",
    "                        condition=\"existing.name != new.name OR \"\n",
    "                        \"existing.city != new.city OR \"\n",
    "                        \"existing.phone_no != new.phone_no OR \"\n",
    "                        \"existing.maritial_status != new.maritial_status OR \"\n",
    "                        \"existing.gender != new.gender OR \"\n",
    "                        \"existing.DOB != new.DOB OR \"\n",
    "                        \"existing.email != new.email\",\n",
    "            set={\n",
    "                \"is_current\": lit(False),\n",
    "                \"end_date\": current_timestamp(),\n",
    "                 }\n",
    "        ).execute()\n",
    "        #Insert records with new keys (new business keys that never existed)\n",
    "        updated_target_df = deltaTable.toDF().filter(\"is_current = true\").select(\"customer_id\")\n",
    "        insert_df = df_new.join(updated_target_df, on=\"customer_id\", how=\"left_anti\")\n",
    "\n",
    "        #Insert records with new keys (new business keys that never existed)\n",
    "        #df_new = df_new.filter(\"is_active=true\").select(\"customer_id\")\n",
    "        #insert_df = source_df_audit_col.join(updated_target_df, on=\"customer_id\", how=\"left_anti\")\n",
    "\n",
    "        \n",
    "        df_new.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"ingestion_time_formatted\").option(\"overwriteSchema\", \"true\").save(silver_path)\n",
    "\n",
    "        # Create a logging table\n",
    "        log_data = [(bronze_path_day0, silver_path, \"success\", datetime.now())]\n",
    "        log_schema = [\"bronze_path\", \"silver_path\", \"status\", \"timestamp\"]\n",
    "        log_df = spark.createDataFrame(log_data, log_schema)\n",
    "\n",
    "        # Write the log data to a logging table\n",
    "        log_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/Prajwal/Capstone_Project/Silver/Silver_logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c5035c-ca56-40da-b7ed-00068a3805b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read data from the bronze path\n",
    "df_bronze = spark.read.format(\"parquet\").load(bronze_path_day1)\n",
    "\n",
    "# Read data from the silver path\n",
    "df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# Count records in both DataFrames\n",
    "bronze_count = df_bronze.count()\n",
    "silver_count = df_silver.count()\n",
    "\n",
    "# Display the counts\n",
    "print(f\"Bronze record count: {bronze_count}\")\n",
    "print(f\"Silver record count: {silver_count}\")\n",
    "\n",
    "\n",
    "# Display the silver DataFrame\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684861dd-a14c-4624-b896-6b0a042d8347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read from silver path\n",
    "df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "793406b9-fdae-4fa1-9385-60a79006c17e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_silver.filter(col(\"customer_id\").isin('7f97ade2-4720-42c6-ab28-9ebcea043cf2',\n",
    "'633a55b8-0678-49ea-aaa2-6a6a40beb308',\n",
    "'901db134-d9d8-4a55-ae1f-3b879fe8a3c3',\n",
    "'52c44179-17c5-44cb-b86e-3b98a9a98230')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec7c9f2e-c781-481c-8fd6-854cb8264fdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Gold/Capstone_Gold_Customerdetails_initialload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4130dbdb-5634-4fbb-b678-0d5366bd35c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Implementing SCD 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8282b3af-e9a4-49bd-93cb-48a0e531559a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# if DeltaTable.isDeltaTable(spark, silver_path) else df_new.write.format('delta').mode('overwrite').save(silver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c05cd3-08a5-430c-8d5e-0401097801bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Import necessary libraries for SCD Type 2\n",
    "# from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "# # Read existing data from the silver path if it exists\n",
    "# try:\n",
    "#     df_existing = spark.read.format(\"delta\").load(silver_path)\n",
    "# except:\n",
    "#     df_existing = spark.createDataFrame([], df.schema)\n",
    "# # \n",
    "# # Add SCD Type 2 columns to the new data\n",
    "# df_new = df.withColumn(\"is_current\", lit(True)) \\\n",
    "#            .withColumn(\"start_date\", current_timestamp()) \\\n",
    "#            .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "# # Merge new data with existing data to identify changes and insert new records\n",
    "# df_final = df_existing.alias(\"existing\").merge(\n",
    "#     df_new.alias(\"new\"),\n",
    "#     \"existing.customer_id = new.customer_id\"\n",
    "# ).whenMatchedUpdate(\n",
    "#     set={\n",
    "#         \"is_current\": lit(False),\n",
    "#         \"end_date\": current_timestamp()\n",
    "#     }\n",
    "# ).whenNotMatchedInsert(\n",
    "#     values={\n",
    "#         \"customer_id\": \"new.customer_id\",\n",
    "#         \"name\": \"new.name\",\n",
    "#         \"city\": \"new.city\",\n",
    "#         \"phone_no\": \"new.phone_no\",\n",
    "#         \"maritial_status\": \"new.maritial_status\",\n",
    "#         \"gender\": \"new.gender\",\n",
    "#         \"email\": \"new.email\",\n",
    "#         \"DOB\": \"new.DOB\",\n",
    "#         \"age\": \"new.age\",\n",
    "#         \"is_current\": lit(True),\n",
    "#         \"start_date\": current_timestamp(),\n",
    "#         \"end_date\": lit(None).cast(\"timestamp\")\n",
    "#     }\n",
    "# ).execute()\n",
    "\n",
    "# # Write the final DataFrame to the silver path\n",
    "# df_final.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"ingestion_time\").save(silver_path)\n",
    "\n",
    "# # Count updated and newly inserted records\n",
    "# updated_count = df_final.filter(df_final[\"is_current\"] == False).count()\n",
    "# newly_inserted_count = df_final.filter(df_final[\"is_current\"] == True).count()\n",
    "\n",
    "# # Display the counts\n",
    "# print(f\"Updated record count: {updated_count}\")\n",
    "# print(f\"Newly inserted record count: {newly_inserted_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddd7abe7-b695-4b3f-97b8-567584d62683",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Importing necessary libraries\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import *\n",
    "# from pyspark.sql.window import Window\n",
    "\n",
    "# # Define paths for bronze and silver zones\n",
    "# bronze_path = \"/mnt/Prajwal/Capstone_Project/bronze/bankcustomer_source1_day0\"\n",
    "# silver_path = \"/mnt/Prajwal/Capstone_Project/silver/Bankcustomer_source\"\n",
    "\n",
    "# # Read data from the bronze path\n",
    "# df = spark.read.format(\"parquet\").load(bronze_path)\n",
    "\n",
    "# # Drop the last 2 rows of the DataFrame\n",
    "# df = df.limit(df.count() - 2)\n",
    "\n",
    "# # Trim spaces from column names\n",
    "# df = df.toDF(*[c.strip() for c in df.columns])\n",
    "\n",
    "# # Rename columns for consistency and readability\n",
    "# df = df.withColumnRenamed(\"_Customerid\", \"customer_id\") \\\n",
    "#        .withColumnRenamed(\"C1ustomer Name\", \"name\") \\\n",
    "#        .withColumnRenamed(\"City\", \"city\") \\\n",
    "#        .withColumnRenamed(\"Phoneno\", \"phone_no\") \\\n",
    "#        .withColumnRenamed(\"Maritial_Status\", \"maritial_status\") \\\n",
    "#        .withColumnRenamed(\"Gender\", \"gender\") \\\n",
    "#        .withColumnRenamed(\"EmailAddress\", \"email\")\n",
    "\n",
    "# # Fill null values with default values\n",
    "# df = df.fillna({\n",
    "#     'name': 'Unknown',\n",
    "#     'city': 'Unknown',\n",
    "#     'phone_no': '000-000-0000',\n",
    "#     'maritial_status': 'Unknown',\n",
    "#     'gender': 'Unknown',\n",
    "#     'email': 'noemail@example.com'\n",
    "# })\n",
    "\n",
    "# # Extract area code from phone number\n",
    "# df = df.withColumn(\"area_code\", regexp_extract(col(\"phone_no\"), r\"(\\d{3})\", 1))\n",
    "\n",
    "# # Ensure email is correctly formatted\n",
    "# df = df.withColumn('email', regexp_replace('email', r'[^a-zA-Z0-9@._-]', ''))\n",
    "\n",
    "# # Change ingestion time to the format of yyyy-mm-dd hh:mm:ss\n",
    "# df = df.withColumn('ingestion_time', date_format(col('ingest_time'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# # Drop duplicate records based on customer_id\n",
    "# df = df.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "# # Drop the original ingestion time column\n",
    "# df = df.drop(\"ingest_time\")\n",
    "\n",
    "# # Define a UDF to fix date of birth (DOB) format\n",
    "# from pyspark.sql.functions import udf\n",
    "# from datetime import datetime\n",
    "\n",
    "# def fix_dob(dob_str):\n",
    "#     try:\n",
    "#         dob = datetime.strptime(dob_str, '%d-%b-%y')\n",
    "#         if dob.year > datetime.today().year:\n",
    "#             dob = dob.replace(year=dob.year - 100)\n",
    "#         return dob\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "# fix_dob_udf = udf(fix_dob, DateType())\n",
    "\n",
    "# # Apply the UDF to fix DOB and calculate age\n",
    "# df = df.withColumn(\"DOB\", fix_dob_udf(col(\"DOB\")))\n",
    "# df_src1 = df.withColumn(\"age\", floor(datediff(current_date(), col(\"DOB\")) / 365.25))\n",
    "\n",
    "# # Define paths for bronze source2 \n",
    "# bronze_path = \"/mnt/Prajwal/Capstone_Project/bronze/bankcustomer_source2_day0\"\n",
    "\n",
    "# # Read data from the bronze path\n",
    "# df_src2 = spark.read.format(\"parquet\").load(bronze_path)\n",
    "\n",
    "# # Rename columns for consistency and readability\n",
    "# df_src2 = df_src2.withColumnRenamed(\"CustomerID\", \"customer_id\") \\\n",
    "#        .withColumnRenamed(\"CustomerName\", \"name\") \\\n",
    "#        .withColumnRenamed(\"City\", \"city\") \\\n",
    "#        .withColumnRenamed(\"Phoneno\", \"phone_no\") \\\n",
    "#        .withColumnRenamed(\"MaritalStatus\", \"maritial_status\") \\\n",
    "#        .withColumnRenamed(\"Gender\", \"gender\") \\\n",
    "#        .withColumnRenamed(\"EmailAddress\", \"email\")\n",
    "\n",
    "# # Trim spaces from column names\n",
    "# df_src2 = df_src2.toDF(*[c.strip() for c in df_src2.columns])\n",
    "\n",
    "# # Extract area code from phone number\n",
    "# df_src2 = df_src2.withColumn(\"area_code\", regexp_extract(col(\"phone_no\"), r\"^(\\d{3})\", 1))\n",
    "\n",
    "# # Ensure email is correctly formatted\n",
    "# df_src2 = df_src2.withColumn('email', regexp_replace('email', r'[^a-zA-Z0-9@._-]', ''))\n",
    "\n",
    "# # Change ingestion time to the format of yyyy-mm-dd hh:mm:ss\n",
    "# df_src2 = df_src2.withColumn('ingestion_time', date_format(col('ingest_time'), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# # Drop the original ingestion time column\n",
    "# df_src2 = df_src2.drop(\"ingest_time\")\n",
    "\n",
    "# # Drop duplicate records based on customer_id\n",
    "# df_src2 = df_src2.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "# # Cast DOB to proper format\n",
    "# df_src2 = df_src2.withColumn(\"DOB\", to_date(col(\"DOB\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# # Calculate age\n",
    "# df_src2 = df_src2.withColumn(\"age\", floor(datediff(current_date(), col(\"DOB\")) / 365.25))\n",
    "\n",
    "# df_src2 = df_src2.select('customer_id',\n",
    "#  'name',\n",
    "#  'city',\n",
    "#  'phone_no',\n",
    "#  'maritial_status',\n",
    "#  'gender',\n",
    "#  'DOB',\n",
    "#  'email',\n",
    "#  'area_code',\n",
    "#  'ingestion_time',\n",
    "#  'age')\n",
    "\n",
    "\n",
    "# df = df_src1.union(df_src2)\n",
    "\n",
    "# df = df.withColumn(\"ingestion_time_formatted\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "# # Read existing data from the silver path if it exists\n",
    "# try:\n",
    "#     df_existing = spark.read.format(\"delta\").load(silver_path)\n",
    "# except:\n",
    "#     df_existing = spark.createDataFrame([], df.schema)\n",
    "\n",
    "# # Add SCD Type 2 columns to the new data\n",
    "# df_new = df.withColumn(\"is_current\", lit(True)) \\\n",
    "#            .withColumn(\"start_date\", current_timestamp()) \\\n",
    "#            .withColumn(\"end_date\", lit(None).cast(\"timestamp\"))\n",
    "\n",
    "\n",
    "# df_new.columns"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7825418665770605,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Capstone_Silver_bankcustomer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
