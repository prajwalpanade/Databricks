{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f444bd6e-e514-48cd-9476-f3471f3e9d3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary functions and types\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8321257b-46f0-4b81-a1d3-1efa0503f589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "silver_path = \"/mnt/Prajwal/Capstone_Project/silver/bankcustomer_source\"\n",
    "gold_path = \"/mnt/Prajwal/Capstone_Project/gold/\"\n",
    "\n",
    "df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "customer_df = df.withColumn(\"Effective_Start_Dt\", F.lit(\"2025-05-04\").cast(\"date\")) \\\n",
    "                .withColumn(\"Effective_End_Dt\", F.lit(None).cast(\"date\")) \\\n",
    "                .withColumn(\"Is_Active\", F.lit(\"Y\")) \\\n",
    "                .withColumn(\"status\", F.lit(\"New\")) \\\n",
    "                .withColumn(\"Customer_Key\", F.monotonically_increasing_id().cast(IntegerType()))\n",
    "\n",
    "# Deduplicate the source DataFrame based on customer_id\n",
    "customer_df = customer_df.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "customer_df = customer_df.select(\n",
    "    'customer_id', \n",
    "    'name', \n",
    "    'city', \n",
    "    'phone_no', \n",
    "    'area_code', \n",
    "    'maritial_status', \n",
    "    'gender', \n",
    "    'DOB', \n",
    "    'age', \n",
    "    'email', \n",
    "    'Effective_Start_Dt', \n",
    "    'Effective_End_Dt', \n",
    "    'Is_Active', \n",
    "    'status', \n",
    "    'Customer_Key'\n",
    ")\n",
    "\n",
    "# Create a DeltaTable object for the existing data\n",
    "deltaTable = DeltaTable.forPath(spark, gold_path + \"dim_customer\")\n",
    "\n",
    "# Merge new data with existing data to implement SCD Type 2\n",
    "deltaTable.alias(\"existing\").merge(\n",
    "    customer_df.alias(\"new\"),\n",
    "    \"existing.customer_id = new.customer_id AND existing.Is_Active = 'Y'\"\n",
    ").whenMatchedUpdate(\n",
    "    condition=\"existing.name != new.name OR \"\n",
    "              \"existing.city != new.city OR \"\n",
    "              \"existing.phone_no != new.phone_no OR \"\n",
    "              \"existing.maritial_status != new.maritial_status OR \"\n",
    "              \"existing.gender != new.gender OR \"\n",
    "              \"existing.DOB != new.DOB OR \"\n",
    "              \"existing.email != new.email\",\n",
    "    set={\n",
    "        \"Is_Active\": F.lit(\"N\"),\n",
    "        \"Effective_End_Dt\": F.current_date()\n",
    "    }\n",
    ").whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# Insert records with new keys (new business keys that never existed)\n",
    "updated_target_df = deltaTable.toDF().filter(\"Is_Active = 'Y'\").select(\"customer_id\")\n",
    "insert_df = customer_df.join(updated_target_df, on=\"customer_id\", how=\"left_anti\")\n",
    "\n",
    "# Append the new records to the Delta table\n",
    "insert_df.write.format(\"delta\").mode(\"append\").save(gold_path + \"dim_customer\")\n",
    "\n",
    "print(\"Customer data updated in Gold layer with SCD Type 2 implementation.\")\n",
    "\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS banking.dim_customer USING DELTA LOCATION '{gold_path}dim_customer'\")\n",
    "\n",
    "display(spark.sql(\"DESCRIBE banking.dim_customer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d25036b-c57b-4f14-9dec-6bda3ac65a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from banking.customer_dim;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f483f8-fee1-42a7-bebe-47dcccb1ecc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# SCD 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fe72d7e-73c0-4f4b-812f-92acbb4cf010",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Load the existing dimension table\n",
    "# dim_customer_path = gold_path + \"dim_customer\"\n",
    "# dim_customer_df = spark.read.format(\"delta\").load(dim_customer_path)\n",
    "\n",
    "# # Load the new data from the silver layer\n",
    "# new_data_df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# # Prepare the new data with SCD Type 2 attributes\n",
    "# new_data_prepared_df = new_data_df.withColumn(\"Effective_Start_Dt\", F.lit(\"2025-05-04\").cast(\"date\")) \\\n",
    "#     .withColumn(\"Effective_End_Dt\", F.lit(None).cast(\"date\")) \\\n",
    "#     .withColumn(\"Is_Active\", F.lit(\"Y\")) \\\n",
    "#     .withColumn(\"status\", F.lit(\"New\")) \\\n",
    "#     .withColumn(\"Customer_Key\", F.monotonically_increasing_id().cast(IntegerType()))\n",
    "\n",
    "# # Join the new data with the existing dimension table to find changes\n",
    "# join_condition = [dim_customer_df.customer_id == new_data_prepared_df.customer_id]\n",
    "# changes_df = dim_customer_df.join(new_data_prepared_df, join_condition, \"outer\") \\\n",
    "#     .select(\n",
    "#         new_data_prepared_df[\"*\"],\n",
    "#         dim_customer_df[\"Effective_End_Dt\"].alias(\"existing_Effective_End_Dt\"),\n",
    "#         dim_customer_df[\"Is_Active\"].alias(\"existing_Is_Active\")\n",
    "#     ) \\\n",
    "#     .filter(\n",
    "#         (dim_customer_df.customer_id.isNull()) | \n",
    "#         (dim_customer_df.name != new_data_prepared_df.name) |\n",
    "#         (dim_customer_df.city != new_data_prepared_df.city) |\n",
    "#         (dim_customer_df.phone_no != new_data_prepared_df.phone_no) |\n",
    "#         (dim_customer_df.area_code != new_data_prepared_df.area_code) |\n",
    "#         (dim_customer_df.maritial_status != new_data_prepared_df.maritial_status) |\n",
    "#         (dim_customer_df.gender != new_data_prepared_df.gender) |\n",
    "#         (dim_customer_df.DOB != new_data_prepared_df.DOB) |\n",
    "#         (dim_customer_df.age != new_data_prepared_df.age) |\n",
    "#         (dim_customer_df.email != new_data_prepared_df.email)\n",
    "#     )\n",
    "\n",
    "# # Update the existing records to set Effective_End_Dt and Is_Active\n",
    "# updates_df = changes_df.filter(changes_df.existing_Is_Active == \"Y\") \\\n",
    "#     .withColumn(\"Effective_End_Dt\", F.lit(\"2025-05-03\").cast(\"date\")) \\\n",
    "#     .withColumn(\"Is_Active\", F.lit(\"N\"))\n",
    "\n",
    "# # Insert the new records\n",
    "# new_records_df = changes_df.filter(changes_df.existing_Is_Active.isNull()) \\\n",
    "#     .select(new_data_prepared_df.columns)\n",
    "\n",
    "# # Combine the updates and new records\n",
    "# final_df = updates_df.union(new_records_df)\n",
    "\n",
    "# # Write the final DataFrame to Delta format\n",
    "# final_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(dim_customer_path)\n",
    "\n",
    "# # Refresh the table\n",
    "# spark.sql(f\"REFRESH TABLE banking.dim_customer\")\n",
    "\n",
    "# display(spark.sql(\"SELECT * FROM banking.dim_customer\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7825418665770564,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Capstone_Gold_Customerdetails_initialload",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
