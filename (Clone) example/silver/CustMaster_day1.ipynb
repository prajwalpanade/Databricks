{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f47e110-624c-4abd-9c54-2315f7c60e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12c56707-1303-45b0-a131-f32765772e84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../logs/logs_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb643740-0597-4b87-8ea2-e859446841ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utilities/Futuredate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea5cead-4857-4511-9ac1-d32f33f287cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../utilities/Pastdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5348e314-bfca-487e-b920-485959e78e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "bronze_path = \"/mnt/mock_prajwal/example/bronze/\"\n",
    "silver_path = \"/mnt/mock_prajwal/example/silver/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b27d62d-c861-4d6d-86b4-fb1b6bfc9599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# try block to handle exceptions\n",
    "try:\n",
    "    # check if the bronze path exists\n",
    "    if dbutils.fs.ls(bronze_path):\n",
    "        # get the list of files in the bronze path\n",
    "        files = [file.name for file in dbutils.fs.ls(bronze_path)]\n",
    "        \n",
    "        # check if 'CustMaster_day1/' exists in the files\n",
    "        if 'CustMaster_day1/' in files:\n",
    "            # set file path and other file details\n",
    "            file_path = bronze_path + \"CustMaster_day1\"\n",
    "            file_name = os.path.basename(file_path)\n",
    "            file_extension = file_name.split(\".\")[-1]\n",
    "            file_name = file_name.split(\".\")[0]\n",
    "            file_info = dbutils.fs.ls(file_path)[0]\n",
    "            file_size_kb = file_info.size / 1024\n",
    "            file_mod_time = datetime.fromtimestamp(file_info.modificationTime / 1000)\n",
    "            processed_by = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user')\n",
    "            file_type = \"parquet\"  # Define file_type\n",
    "            Layer = \"silver\"\n",
    "            \n",
    "            # start time of the files\n",
    "            start_time = time.time()\n",
    "            log_message(file_path, file_type, file_size_kb, file_mod_time, None, \"PROCESSING\", 0, processed_by, f\"Reading Parquet File {file_name}\", Layer)\n",
    "            \n",
    "            # Load the parquet file into a DataFrame\n",
    "            df = spark.read.format(\"parquet\").load(bronze_path + \"CustMaster_day1\")\n",
    "\n",
    "            #lower all column and trim spaces from column\n",
    "            df = df.toDF(*[c.replace(\" \", \"\") for c in df.columns])\n",
    "            df = df.toDF(*[c.lower().replace(\" \", \"_\") for c in df.columns])\n",
    "\n",
    "            pastdate = udf(date_format_udf, DateType())\n",
    "            futuredate = udf(date_format_udf_Policy_future, DateType())\n",
    "\n",
    "            df = df.withColumn(\"dob\", pastdate(col(\"dob\")))\n",
    "            df = df.withColumn(\"join_date\", futuredate(col(\"join_date\")))\n",
    "\n",
    "            # Ensure email is correctly formatted\n",
    "            df = df.withColumn('email', regexp_replace('email', r'[^a-zA-Z0-9@._-]', ''))\n",
    "\n",
    "            # Standardize phone numbers (Ensuring numeric values)\n",
    "            df = df.withColumn(\"phone_number\", regexp_replace(col(\"phone_number\"), \"[^0-9]\", \"\"))\n",
    "\n",
    "            df = df.withColumn(\"dob\", to_date(coalesce(col(\"dob\"), lit(\"1999-01-01\")), \"yyyy-MM-dd\"))\n",
    "            df = df.withColumn(\"join_date\", to_date(coalesce(col(\"join_date\"), lit(\"2020-01-01\")), \"yyyy-MM-dd\"))\n",
    "\n",
    "            df = df.withColumn(\"address\", coalesce(col(\"address\"), lit(\"unknown\")))\n",
    "\n",
    "            # Record count and processing time\n",
    "            record_count = df.count()\n",
    "            processing_time_sec = int(time.time() - start_time)\n",
    "            \n",
    "            # Final status\n",
    "            log_message(file_path, file_type, file_size_kb, file_mod_time, record_count, \"COMPLETED\", processing_time_sec, processed_by, f\"Successfully processed {file_path}\", Layer)\n",
    "\n",
    "            df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"ingestion_time\").option(\"overwriteSchema\", \"true\").save(silver_path + \"CustMaster_day1\")\n",
    "# handle exceptions\n",
    "except Exception as e:\n",
    "    processing_time_sec = int(time.time() - start_time)\n",
    "    log_message(file_path, file_type, file_size_kb, file_mod_time, 0, \"FAILED\", processing_time_sec, processed_by, f\"Error processing file {file_path}: {str(e)}\", Layer)\n",
    "    raise e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dee462a-9274-47f2-93aa-237e78755b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the data from the silver layer\n",
    "df_silver = spark.read.format(\"delta\").load(silver_path + \"CustMaster_day1\")\n",
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6a7528-2203-4d39-89c4-62b72a4be54d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_path = \"/mnt/mock_prajwal/example/logs\"\n",
    "df_logs = spark.read.format(\"delta\").load(log_path)\n",
    "df_logs_today = df_logs.filter(df_logs['processed_time'].cast(\"date\") == \"2025-05-21\")\n",
    "display(df_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac4ccac-5381-4c65-bf18-5fb292b5be4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the data from the bronze layer\n",
    "df_bronze = spark.read.format(\"parquet\").load(bronze_path + \"CustMaster_day1\")\n",
    "\n",
    "# Count the records in bronze and silver layers\n",
    "bronze_count = df_bronze.count()\n",
    "silver_count = df_silver.count()\n",
    "\n",
    "# Display the counts\n",
    "counts_df = spark.createDataFrame([(bronze_count, silver_count)], [\"bronze_count\", \"silver_count\"])\n",
    "display(counts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25f95fbc-867c-4272-a1e8-fca0400c7380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6d59a13-edab-4a3f-ade7-3396e54cfae9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65220dc2-7260-4bfa-9bd8-f5bce88f6f3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69a65c89-4072-4e4d-86b7-84b38146ad43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df = df.toDF(*[c.replace(\" \", \"\") for c in df.columns])\n",
    "# df = df.toDF(*[c.lower().replace(\" \", \"_\") for c in df.columns])\n",
    "\n",
    "# pastdate = udf(date_format_udf, DateType())\n",
    "# futuredate = udf(date_format_udf_Policy_future, DateType())\n",
    "\n",
    "# df = df.withColumn(\"dob\", pastdate(col(\"dob\")))\n",
    "# df = df.withColumn(\"join_date\", futuredate(col(\"join_date\")))\n",
    "\n",
    "# # Ensure email is correctly formatted\n",
    "# df = df.withColumn('email', regexp_replace('email', r'[^a-zA-Z0-9@._-]', ''))\n",
    "\n",
    "# # Standardize phone numbers (Ensuring numeric values)\n",
    "# df = df.withColumn(\"phone_number\", regexp_replace(col(\"phone_number\"), \"[^0-9]\", \"\"))\n",
    "\n",
    "# df = df.withColumn(\"dob\", to_date(coalesce(col(\"dob\"), lit(\"1999-01-01\")), \"yyyy-MM-dd\"))\n",
    "# df = df.withColumn(\"join_date\", to_date(coalesce(col(\"join_date\"), lit(\"2020-01-01\")), \"yyyy-MM-dd\"))\n",
    "\n",
    "# df = df.withColumn(\"address\", coalesce(col(\"address\"), lit(\"unknown\")))\n",
    "# display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CustMaster_day1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
