{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52ae609b-c0a1-4504-b4b0-67c7fc8fd504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import monotonically_increasing_id, col, count, row_number, lit, current_timestamp, coalesce, max\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f36e642-b017-48b5-b386-26e62ef87c58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Paths\n",
    "silver_path = \"/mnt/mock_prajwal/example/silver/\"\n",
    "gold_path = \"/mnt/mock_prajwal/example/gold/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3913c56a-1767-440f-8842-f5e0cff4f909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(silver_path + \"CustMaster\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c1b1505-76bb-46b2-87c3-ce46149c6ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea86806-c718-439d-a200-2d80e6ec26e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "df_selected = df.select(\n",
    "    col(\"customer_id\"),\n",
    "    col(\"customer_name\"),\n",
    "    col(\"dob\"),\n",
    "    col(\"gender\"),\n",
    "    col(\"email\"),\n",
    "    col(\"phone_number\"),\n",
    "    col(\"address\"),\n",
    "    col(\"city\"),\n",
    "    col(\"state\"),\n",
    "    col(\"country\"),\n",
    "    col(\"customer_segment\"),\n",
    "    col(\"join_date\")\n",
    ").dropDuplicates(['customer_id'])\n",
    "\n",
    "window_spec = Window.orderBy(\"customer_id\")\n",
    "\n",
    "df_selected = df_selected.withColumn(\"customer_key\", row_number().over(window_spec))\n",
    "\n",
    "display(df_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff1f5a6d-571e-4b56-b829-3b09aa17b951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"dob\", DateType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone_number\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"customer_segment\", StringType(), True),\n",
    "    StructField(\"join_date\", DateType(), True),\n",
    "    StructField(\"customer_key\", IntegerType(), False),\n",
    "    StructField(\"start_date\", TimestampType(), False),\n",
    "    StructField(\"end_date\", TimestampType(), True),\n",
    "    StructField(\"is_active\", BooleanType(), False),\n",
    "    StructField(\"last_modified\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "# Create the schema if it does not exist\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS Prajwal_Telecom\")\n",
    "\n",
    "# Create the table with the specified schema\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS Prajwal_Telecom.Dim_Customer (\n",
    "        customer_id INT,\n",
    "        customer_name STRING,\n",
    "        dob DATE,\n",
    "        gender STRING,\n",
    "        email STRING,\n",
    "        phone_number STRING,\n",
    "        address STRING,\n",
    "        city STRING,\n",
    "        state STRING,\n",
    "        country STRING,\n",
    "        customer_segment STRING,\n",
    "        join_date DATE,\n",
    "        customer_key INT NOT NULL,\n",
    "        start_date TIMESTAMP NOT NULL,\n",
    "        end_date TIMESTAMP,\n",
    "        is_active BOOLEAN NOT NULL,\n",
    "        last_modified TIMESTAMP NOT NULL\n",
    "    )\n",
    "        USING DELTA \n",
    "    LOCATION \"/mnt/mock_prajwal/example/gold/Dim_Customer\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9e4bd83-df2d-46db-a6b1-d9e0d239ed71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, max, lit, row_number, col, current_timestamp\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load target table\n",
    "target_df = spark.read.table(\"Prajwal_Telecom.Dim_Customer\")\n",
    "\n",
    "# Check if 'customer_key' column exists in the target table\n",
    "if 'customer_key' in target_df.columns:\n",
    "    # Finding max of previous data from table\n",
    "    max_key = target_df.agg(coalesce(max(\"customer_key\"), lit(0))).collect()[0][0]\n",
    "else:\n",
    "    max_key = 0\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.orderBy(\"customer_id\")\n",
    "\n",
    "# Add row number and customer_key\n",
    "source_df_keyed = df_selected \\\n",
    "    .withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"customer_key\", col(\"rn\") + lit(max_key)) \\\n",
    "    .drop(\"rn\")\n",
    "\n",
    "# Add audit columns to source\n",
    "source_df_audit_col = source_df_keyed \\\n",
    "    .withColumn(\"start_date\", current_timestamp()) \\\n",
    "    .withColumn(\"end_date\", lit(None).cast(\"timestamp\")) \\\n",
    "    .withColumn(\"is_active\", lit(True)) \\\n",
    "    .withColumn(\"last_modified\", current_timestamp())\n",
    "\n",
    "# Convert target to DeltaTable\n",
    "target_table = DeltaTable.forName(spark, \"Prajwal_Telecom.Dim_Customer\")\n",
    "\n",
    "# Merge condition (on business key and active rows)\n",
    "merge_condition = \"target.customer_id = source.customer_id AND target.is_active = true\"\n",
    "\n",
    "# Perform Expiration\n",
    "target_table.alias(\"target\") \\\n",
    "    .merge(source_df_audit_col.alias(\"source\"), merge_condition) \\\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"target.customer_name != source.customer_name OR \"\n",
    "                  \"target.dob != source.dob OR \"\n",
    "                  \"target.gender != source.gender OR \"\n",
    "                  \"target.email != source.email OR \"\n",
    "                  \"target.phone_number != source.phone_number OR \"\n",
    "                  \"target.address != source.address OR \"\n",
    "                  \"target.city != source.city OR \"\n",
    "                  \"target.state != source.state OR \"\n",
    "                  \"target.country != source.country OR \"\n",
    "                  \"target.customer_segment != source.customer_segment\",\n",
    "        set={\n",
    "            \"end_date\": current_timestamp(),\n",
    "            \"is_active\": lit(False),\n",
    "            \"last_modified\": current_timestamp()\n",
    "        }\n",
    "    ).execute()\n",
    "\n",
    "# Insert records with new keys (new business keys that never existed)\n",
    "updated_target_df = spark.read.table(\"Prajwal_Telecom.Dim_Customer\").filter(\"is_active=true\").select(\"customer_id\")\n",
    "insert_df = source_df_audit_col.join(updated_target_df, on=\"customer_id\", how=\"left_anti\")\n",
    "\n",
    "insert_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"/mnt/mock_prajwal/example/gold/Dim_Customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec3de5f-d35a-4c97-b9bd-b2e390c7cffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dim_customer = spark.read.format(\"delta\").load(\"/mnt/mock_prajwal/example/gold/Dim_Customer\")\n",
    "display(dim_customer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c5be81e-caca-443e-bb4c-12901fb39c12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load silver layer table\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path + \"CustMaster\")\n",
    "\n",
    "# Load gold layer table\n",
    "gold_df = spark.read.format(\"delta\").load(\"/mnt/mock_prajwal/example/gold/Dim_Customer\")\n",
    "\n",
    "# Record count for silver layer\n",
    "silver_count = silver_df.count()\n",
    "\n",
    "# Record count for gold layer\n",
    "gold_count = gold_df.count()\n",
    "\n",
    "# Display counts\n",
    "display(spark.createDataFrame([(silver_count, gold_count)], [\"Silver Layer Count\", \"Gold Layer Count\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c790452-7a04-401d-9f07-6c6931365002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of updated records\n",
    "updated_count = target_table.toDF().filter(\"is_active = false AND last_modified = current_timestamp()\").count()\n",
    "\n",
    "# Count the number of inserted records\n",
    "inserted_count = insert_df.count()\n",
    "\n",
    "# Display the counts\n",
    "display(spark.createDataFrame([(updated_count, inserted_count)], [\"Updated Records\", \"Inserted Records\"]))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dim_CustMaster",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
