{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5c4b55-9616-4469-aae4-5f5d4019aa79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608a9ad6-7823-44f4-b1f4-7fcb1ee020a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_path = \"/mnt/mock_prajwal/bronze/logs\"\n",
    "\n",
    "# Define the enhanced Log Table Schema\n",
    "log_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"log_level\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True),\n",
    "    StructField(\"file_type\", StringType(), True),\n",
    "    StructField(\"file_size_kb\", DoubleType(), True),\n",
    "    StructField(\"file_mod_time\", TimestampType(), True),\n",
    "    StructField(\"record_count\", IntegerType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"processing_time_sec\", IntegerType(), True),\n",
    "    StructField(\"processed_by\", StringType(), True),\n",
    "    StructField(\"processed_time\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Function to log message with additional metadata\n",
    "def log_message(source_file, file_type, file_size_kb, file_mod_time, record_count, status, processing_time_sec, processed_by, message):\n",
    "    log_entry = [(datetime.now(), \"INFO\", message, file_type, file_size_kb, file_mod_time, record_count, status, processing_time_sec, processed_by, datetime.now())]\n",
    "    log_df = spark.createDataFrame(log_entry, log_schema)\n",
    "    log_df.write.format(\"delta\").mode(\"append\").save(log_path)\n",
    "    print(f\"{datetime.now()} - [{message}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a4ec7cd-fc12-41c3-95be-9d77f6f6ec2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "logs_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
